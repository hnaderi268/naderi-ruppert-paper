% !TEX root =  podc-submission.tex

\subsection{Correctness}
Unfortunately, there are enough changes to the algorithm that a new proof of correctness
is required.  Its structure mirrors the proof of the original algorithm, but requires additional
reasoning to ensure GC does not interfere with other routines.
The following observation describes
how the set of blocks in the RBT can be modified.

\begin{lemma}\label{RBTupdates}
Suppose a step of the algorithm changes $v.\fld{blocks}$ from a non-empty tree $T$ to $T'$.
If the set of \fld{index} values  in $T$ is $I$, then the set of \fld{index} values in $T'$ is 
$(I \cap [m-1,\infty))\cup \{\max(I) + 1\}$ for some $m$.
\end{lemma}
\begin{proof}
The RBT of a node is updated only at line \ref{appendLeafGC} or \ref{casGC}.

If line \ref{appendLeafGC} of an \op{Append} operation modifies $v.\fld{blocks}$,
then $v$ is a leaf node, and no other process ever modifies $v.\fld{blocks}$.
$T'$ was obtained from $T$ by calling \op{AddBlock}($T,B$).
$B$ was created either by the \op{Enqueue} or \op{Dequeue} that called \op{Append}.
Either way, $B.\fld{index} = \max(I)+1$.
The \op{AddBlock} that creates $T'$ may optionally \op{Split} the RBT and then add $B$ to it.
So the claim is satisfied.

If line \ref{casGC} of a \op{Refresh} modifies $v.\fld{blocks}$, then $v$ is an internal node.
After reading $T$ from $v.\fld{blocks}$ at line \ref{refreshReadTGC},
then creates the  block \var{new},
and then calls \op{AddBlock}($T,new$) to create $T'$.
Line \ref{createIndexGC} sets $\var{new}.\var{index} = \max(I)+1$.
The \op{AddBlock} that creates $T'$ may optionally \op{Split} the RBT and then add $new$ to it.
So the claim is satisfied.
\end{proof}

Since each RBT starts with a single block with \fld{index} 0, the following is an easy consequence of \Cref{RBTupdates}.
\begin{corollary}\label{nonEmptyIncreasing}
The RBT stored in each node $v$ is never empty and always stores a set of blocks with consecutive indices.
Moreover, its maximum \fld{index} can only increase over time.
\end{corollary}

Since RBTs are always non-empty, calls to \op{MaxBlock} have well-defined answers.  
Throughout the proof, we use $v.\blocks[b]$ to refer to the block with \fld{index} $b$ that appeared
in $v$'s tree at some time during the execution.
It follows from \Cref{RBTupdates} and \Cref{nonemptyConsec} that 
each time a new block appears in $v$'s RBT, its \fld{index} is greater than any block
that has appeared in $v$'s RBT earlier.
Thus, $v.\blocks[b]$ is unique, if it exists.
We also use this notation in the code to indicate that a search of the RBT should be performed for the block
with \fld{index} $b$.
We describe later what should happen if that block is not found by the search because it has been removed
by a GC phase.
\here{remember to come back to this point}

We now establish that definition (\ref{defsubblock}) of a block's subblocks
still makes sense.

%\here{If next obs is only used once, can fold it into the proof that needs it}
%\begin{observation}
%\label{edirGC} 
%If $B$ is a block in the RBT of an internal node $v$, then for $\var{dir} \in \{\fld{left},\fld{right}\}$,
%$B.\edir$ was read from the \fld{index} field of some node in the RBT of $v.\var{dir}$.
%\end{observation}
%\begin{proof}
%The \eleft\ and \eright\ fields are set by reading the \fld{index} field of a node in the appropriate child
%on line \ref{createEndLeftGC} and \ref{createEndRightGC}.
%\end{proof}

We next prove the analogue of Lemma \ref{lem::headProgress}.
\begin{customlemma}{\ref{lem::headProgress}$'$}\label{lem::headProgressGC}
 If $h>0$ and a block with index $h$ has been inserted into $v.\fld{blocks}$ then 
 $v.\fld{blocks}[h-1].\fld{end\sub{left}} \leq v.\fld{blocks}[h].\fld{end\sub{left}}$ and 
 $v.\fld{blocks}[h-1].\fld{end\sub{right}} \leq v.\fld{blocks}[h].\fld{end\sub{right}}$.
\end{customlemma}
\begin{proof}
The block $B$ with index $h$ was installed into $v$'s RBT by the CAS at line \ref{casGC} 
that changed the tree from $T$ to $T'$.
Before this CAS, line \ref{refreshReadTGC} read the tree $T$ from $v.blocks$,
line \ref{refreshReadMax} found a block $B'$  with \fld{index} $h-1$ in $T$,
and then line \ref{invokeCreateBlockGC} created the block $B$ with $\fld{index}=h$.
Since $B'$ was already in $T$ before $B$ was created, the
\op{CreateBlock}$(v,b-1)$ that created $B'$ terminated before the 
\op{CreateBlock}$(v,b)$ that created $B$ started.
By \Cref{nonEmptyIncreasing}, the value that line \ref{createEndLeftGC} of \op{CreateBlock}$(v,b-1)$ 
stores in $B'.\eleft$ is less than or equal to the value that line \ref{createEndLeftGC} 
of \op{CreateBlock}$(v,b)$ stores in $B.\eleft$.  
Similarly, the values stored in $B'.\eright$ and $B.\eright$ at line \ref{createEndRightGC} satisfy the claim.
\end{proof}

\Cref{lem::subblocksDistinct}, \Cref{lem::noDuplicates}, \Cref{lem::sum}, \Cref{blockNotEmpty} 
hold for the modified algorithm.  Their proofs are identical to those given in 
Section \ref{sec::basicProperties} since they  depend only on \Cref{lem::headProgress} (which can be replaced by \Cref{lem::headProgressGC})
and the definition of subblocks given in (\ref{defsubblock}).
Next, we prove an analogue of \Cref{successfulRefresh}.
We say a node $v$ \emph{contains} an operation
if some block containing the operation has previously appeared in $v$'s RBT (even if 
the block has been removed from the RBT by a subsequent \op{Split}).

\begin{customlemma}{\ref{successfulRefresh}$'$}
\label{successfulRefreshGC}
Let $R$ be a call to \op{Refresh}($v$) that performs a successful \op{CAS} on line \ref{casGC} (or terminates at line \ref{addOPGC}).
In the configuration after that CAS (or termination, respectively), $v$ contains all operations that $v$'s children contained 
when $R$ executed line~\ref{refreshReadTGC}.
\end{customlemma}
\begin{proof}
Suppose $v$'s child (without loss of generality, $v.\fld{left}$) contained an operation $op$ 
when $R$ executed line \ref{refreshReadTGC}.
Let $i$ be the index of the block containing $op$ that was in $v$'s RBT before 
$R$ executed line \ref{refreshReadTGC}.
We consider two cases.

Suppose $R$'s call to \op{CreateBlock} returns a new block $B'$.  Then, $R$'s \op{CAS} at line \ref{cas} 
installs $B'$ in $v.\fld{blocks}$.
The \op{CreateBlock} set $B'.\eleft$ to the maximum \fld{index} in $v.\fld{left}$'s RBT at line \ref{createEndLeftGC}.
By \Cref{nonEmptyIncreasing}, this maximum \fld{index} is bigger than $i$.
By the definition of subblocks, some block in $v$ contains $B$ as a subblock
and therefore $v$ contains $op$.

Now suppose $R$'s call to \op{CreateBlock} returns \nl, causing $R$ to terminate at line \ref{addOPGC}.
Let $h$ be the maximum \fld{index} in $T$ plus 1.
By reasoning identical to the last paragraph of \Cref{successfulRefresh}'s proof,
it follows from the fact that $\var{num\sub{enq}}+\var{num\sub{deq}}=0$ at line \ref{testEmptyGC}
that the blocks $v.\fld{left.blocks}[v.\fld{blocks}[h-1].\eleft+1..new.\eleft]$ and
$v.\fld{right.blocks}[v.\fld{blocks}[h-1].\eright+1..new.\eright]$
contain no operations.
By \Cref{blockNotEmpty}, each block contains at least one operation, so
these ranges must be empty, and $v.\fld{blocks}[h-1].\eleft \geq \var{new}.\eleft\geq i$.
This implies that the block $B$ containing $op$ is a subblock of some block that has appeared
in $v$'s RBT, so $op$ is contained in $v$.
\end{proof}

This allows us to show that a double \op{Refresh} propagates operations up the tree, 
as in \Cref{lem::doubleRefresh}.

\begin{customlemma}{\ref{lem::doubleRefresh}$'$}\label{lem::doubleRefreshGC}
Consider two consecutive terminating calls $R_1$, $R_2$ to \op{Refresh}($v$) by the same process.
All operations contained $v$'s children when $R_1$ begins
are contained in $v$ when $R_2$ terminates.
\end{customlemma}
\begin{proof}
If either $R_1$ or $R_2$ performs a successful \op{CAS} at line \ref{casGC} or terminates at line \ref{addOPGC}, the claim follows
from \Cref{successfulRefreshGC}.
So suppose both $R_1$ and $R_2$ perform a failed \op{CAS} at line \ref{casGC}.
\here{To be continued....}

\end{proof}



\here{next, should probably show that blocks are removed only after they are no longer needed; i.e., every dequeue has its answer (either returned, or written in its response field) and every enqueue has a matching dequeue that has either terminated or has a response written.}


-------

\subsection{Analysis}
\label{sec::GCanalysis}

\here{Danger:  what if I remove a block (by a split), which prevents another process from continuing on to mark ancestors/descendants of this node?  Does existing helping take care of this?  Or maybe this does not cause too many excess blocks to be left around?  If not, maybe before splitting, we should mark descendants/ancesotrs of max marked block; in this case, all marking should go in the same direction up/down tree.}

We first bound the size of RBTs.  Let $q_{max}$ be the maximum size of the queue at any time during the sequential execution given by the linearization $L$.
Recall that GC is done on a node every $G$ times its RBT is updated, where we chose $G$ to be $p^2\ceil{\log p}$.

We say the index $b$ \emph{is marked in node $v$} if some process has tested
whether $b>\var{v}.\fld{last}[*]$ at line \ref{marktest1} or \ref{marktest2}.
Intuitively, a block's index value is marked 
when a dequeue passes through this block to find the enqueue
whose argument it should return, or when a dequeue that returns null passes through the block.
Thus, $b$ gets marked in $v$ when $v.\fld{blocks}[b]$ 
contains either an enqueue whose value is being dequeued or a \nl\ dequeue.
If the index of block $B$ is marked in $v$, we also say $B$ is marked in $v$.

\begin{lemma}\label{boundAfterGC}
If the maximum \fld{index} in a node's RBT is a multiple of $G$, then
it contains $O(q_{max}+p)$ blocks.
\end{lemma}
\begin{proof}
Consider the \op{AddBlock} that updates a node $v$'s RBT with the insertion of a block whose 
\fld{index} is a multiple of $G$.
That \op{AddBlock} performs a GC phase.
Let $C$ be the configuration after it executes line \ref{beforeLastScan}.
Let $L'$ be the prefix of the linearization order $L$ consisting of blocks that have beed added to the
root before $C$.
$A$'s \op{Split} on line \ref{splitGC} removes from the RBT all blocks whose indices
were marked in $v$ prior to line \ref{beforeLastScan}, except for $m$ and $m-1$.
To bound the number of blocks left in the RBT after the split, 
we get a bound on the number of operations in unmarked blocks in $v$.

First consider an enqueue operation $E$ in an unmarked block in $v$.
No dequeue that returns $E$'s value has completed (otherwise it would have marked the block in $v$).
So, if the dequeue is in $L'$, then the dequeue is still pending at $C$.
There can be at most $p$ dequeues pending at $C$.
So, either $E$'s element is still in the 

An enqueue operation can be in an unmarked block in $v$
only if the dequeue that returns its value has not yet completed.


\here{to be continued}
\end{proof}


\begin{corollary}\label{RBTbound}
At all times, the size of a node's RBT is $O(q_{max}+p+G)$. 
\end{corollary}
\begin{proof}
Each update to a node's RBT adds at most one block to it, increasing its maximum \fld{index} by 1.
Thus, there are at most $G$ updates since the last time its maximum \fld{index} was a multiple of $G$.
The claim follows from \Cref{boundAfterGC}.
\end{proof}

\begin{theorem}\label{spaceBound}
The size of our queue data structure is $O(pq_{max}+p^3\log p)$ words of memory.
\end{theorem}
\begin{proof}
There are $2p-1$ nodes in the ordering tree.  Aside from the RBT, each node uses $O(p)$ memory for the
\fld{last} array.  So the space bound follows from \Cref{RBTbound} and the fact that $G$ is chosen
to be $p^2\ceil{\log p}$.
\end{proof}

Although individual operations may now become more expensive because they have to perform GC,
we show that operations still have polylogarithmic amortized step complexity.

\begin{proposition}
The amortized step complexity is $O(\log p \log(p+q_{max}))$ per operation.
\end{proposition}
\begin{proof}
It follows from \Cref{RBTbound} and the fact that $G=p^2\ceil{\log p}$
that all the RBT routines we use to perform \op{Split}, \op{Insert} and searches for
blocks with a particular index or for a \fld{sum\sub{enq}} value (in line \ref{FRsearchGC} or \ref{getChildGC}) can all be done in $O(\log(p+q_{max}))$ steps.

First, we bound the number of steps taken \emph{excluding} the GC phase in line \ref{GCstart}--\ref{GCend}.
An \op{Enqueue} or null \op{Dequeue} does $O(1)$ RBT operations and other work at each level of the tree during \op{Propagate},
for a total of $O(\log p \log(p+q_{max}))$ steps.
A non-null \op{Dequeue} must also search for a block in the root at line \ref{FRsearchGC}
and call \op{GetEnqueue}.  At each level of the tree, \op{GetEnqueue} does $O(1)$ RBT operations (including a search at line \ref{getChildGC}) and $O(1)$ other steps.
Thus, a \op{Dequeue} also takes $O(\log p \log(p+q_{max}))$ steps.

Now we consider the additional steps a process takes while doing GC in line \ref{GCstart}--\ref{GCend}.
If a process does GC in a call to \op{AddBlock}($v,T,B$) where $B$ has \fld{index} $r\cdot G$ for some integer $r$, we call this the process's \emph{$r$th helping phase on $v$}.
We argue that each process $P$ can do an $r$th helping phase on $v$ at most once as follows.
Consider $P$'s first call $A$ to \op{AddBlock} that does an $r$th helping phase on $v$.
Let $v,T,B$ be the arguments of $A$.
Any call to \op{AddBlock} on internal node $v$ is from line \ref{refreshABGC} of \op{Refresh}, so $B.\fld{index}$
is the maximum \fld{index} in $T$ plus 1.
The \op{Refresh} that called $A$ performed a \op{CAS} at line \ref{casGC}.  Either the \op{CAS} succeeds
or it fails because 
some other \op{CAS} changes $v.blocks$ from $T$ to another tree.
Either way, by \Cref{RBTupdates}, $v$'s RBT's maximum \fld{index} will be at least $rG$ at all times
after this CAS.
So if a subsequent \op{Refresh} by process $P$ 
ever calls \op{AddBlock} on $v$ again, the block it passes as the third argument
will have $\index>rG$, so $P$ will not perform an $r$th helping phase on $v$ again.

Each helping phase takes $O(p)$ steps to read the \var{last} array,
$O(p \log p \log(p+q_{max}))$ steps to \op{Help},
followed by $O(\log(p+q_{max}))$ steps to split and insert a new node into a RBT.
Thus, for each integer $r$ and each node $v$, a total of $O(p^2\log p\log(p+q_{max}))$ steps
are performed by all processes during their $r$th helping phase on $v$.
We can amortize these steps over the operations that appear in 
$v.blocks[(r-1)p^2+1..p^2]$.
By \Cref{blockNotEmpty}, there are at least $G$ such operations, 
so each operation's amortized number of steps for GC at each node along the path from its leaf to the root
is $O(p^2\log p\log(p+q_{max})/G)=O(\log(p+q_{max}))$.
Hence each operation's amortized number of GC steps is $O(\log p\log(p+q_{max}))$, which is
within a constant factor of the steps it performs excluding GC.
\end{proof}


============

In our design a process attempts to collect the garbage, when it is going to to append a block in the $kp^2$ position in \nf{root.blocks} (see Line \ref{}). Every $p^2$  block appended to the root, one \nf{GarbageCollect} terminates because \nf{root.head} cannot advance until a \nf{GarbageCollect} garbage collect is done (see Lines ).

%\begin{lemma}
%Size of \nf{root.blocks} after \nf{GarbageCollect} is $O(p^2+q)$.
%\end{lemma}
%
%\begin{lemma}
%Total number of the blocks in the tree is $O(p^3+pq)$.
%\end{lemma}

\nf{Enqueue} operation $e$ can be removed from the tree after termination of \nf{Dequeue} $d$ where $Resp(d)=e$. It is safe to remove \nf{Dequeue} $d$ from the tree after the $d$ terminates. We can remove a block after the conditions told are satisfied for all of its \nf{Enqueue}s and \nf{Dequeue}s. A \nf{Dequeue} may go to sleep for a long time and prevent a \nf{block} to be removed (the block it is in or the block its response is in). In that situation other processes can help the \nf{Dequeue} by computing its response and writing it down somewhere. After writing down the response of th \nf{Dequeue} can be removed, since if the process wanted the response it can read from the helped response.

\begin{definition}
A block  is \it{finished} if all of its \nf{Enqueue}s have been computed to be the response of some \nf{Dequeue} and all of its \nf{Dequeues} responses have been computed.  
\end{definition}

\begin{corollary}
If a block is finished, then all of its subblocks are also finished.  
\end{corollary}

\begin{lemma}
It is safe to remove a finished block from the non-leaf nodes.
\end{lemma}

If the $i$th \nf{Enqueue} gets dequeued in a FIFO queue, it means the first $i-1$ \nf{Enqueue} operations have been already dequeued. This gives us the idea that if a block is finished then all the blocks before it are also finished. If an operation in a block goes to sleep for a long time then other processes help the operation so the block gets finished. There are less than $p$ idle operations, we can help them before garbage collection and then remove all the finished blocks safely in an amortized poly-log time.

\begin{lemma}
    If all \nf{Dequeue} operations in the root are helped, then all the blocks before a finished block in the root are also finished.
\end{lemma}

\begin{lemma}
  Blocks before the most recent block that has been dequeued(computed to be dequeue) from are finished. If the most current \nf{Dequeue} returned(computed) \nf{null} then all the blocks before the block containing the \nf{Dequeue} are finished.
\end{lemma}

The idea above leads us to a poly-log data structure that supports throwing away all the blocks with keys smaller than an index. Red-black trees do this for us. \nf{Get(i)}, \nf{Append()} and \nf{Split(i)} are logarithmic in block trees.
We can create a shared red-black tree just creating a new path for the operation and then using \nf{CAS} to change the root of the tree. See [this] for more.

\begin{observation}
PBRT supports poly-log operations ....
\end{observation}

\begin{lemma}
    If we replace the arrays we used to implement \nf{blocks} with red-black trees the amortized complexity of the algorithm would be $PolyLog(p,q)$. And also the algorithm is correct.
\end{lemma}

We can help a \nf{Dequeue} by computing its response and writing it down. If the process in future failed to execute, it can read the helped value written down.

\begin{lemma}
The \nf{response} written is correct.
\end{lemma}

But how can we know which blocks in each node are finished or not? 

\begin{observation}
Every $p^2$ block appended to the root, \nf{FreeMemory} is invoked.  
\end{observation}

To know the last block dequeied from we can implement a shared array among processes which they write the last root block they have dequeued from. 

\begin{lemma}
    $\nf{Max(Last)} - \text{index of the last finished block}$ in the node $n$ is $O(p)$.
\end{lemma}

\begin{lemma}
    After \nf{FreeMemory}, the space taken by each node of the tree is $O(p+q)$.
\end{lemma}

\begin{corollary}
The space taken by each node of the tree is $O(p^2+q)$. The total space in the tree is $PolyLog(p+q)$.
\end{corollary}

\begin{lemma}
  The amortized step per process for the algorithm with garbage collection is $PolyLog(p+q)$.
\end{lemma}

\begin{lemma}
  Algorithm is wait-free and linearizable.
\end{lemma}

\here{Remark on an optimization:  instead of storing \head\ field separate from \fld{blocks}, we could store
the maximum index of the RBT in the root of the RBT.  This would simplify things, but require modifying the proof more extensively.}

