\documentclass[10pt]{article}
\renewcommand{\baselinestretch}{1.8}

\usepackage[b4paper,left=0.8in,right=0.8in,top=0.8in,bottom=0.8in]{geometry}
\usepackage{tikz-qtree}
\usepackage{algorithm}
\usepackage{algpseudocode}
\makeatletter
\renewcommand{\ALG@beginalgorithmic}{\footnotesize}
\makeatother
\usepackage{graphicx}
\usepackage{subcaption}
%\usepackage{showkeys}
\usepackage{amsmath}


\usepackage{multicol}
\setlength\columnsep{24pt}

\algnewcommand\algorithmicforeach{\bf{for each}}
\algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

\algdef{S}[FUNCTION]{Function}
   [3]{{\tt{\sl{#1}}} \textproc{\tt{#2}}\ifthenelse{\equal{#3}{}}{}{\tt{(#3)}}}
  
\algdef{E}[FUNCTION]{EndFunction}
   [1]{\algorithmicend\ \tt{\textproc{#1}}}

\algrenewcommand\Call[2]{\tt{\textproc{#1}\ifthenelse{\equal{#2}{}}{}{(#2)}}}
   
\newcommand\keywordfont{\sffamily\bfseries}
\algrenewcommand\algorithmicend{{\keywordfont end}}
\algrenewcommand\algorithmicfor{{\keywordfont for}}
\algrenewcommand\algorithmicforeach{{\keywordfont for each}}
\algrenewcommand\algorithmicdo{{\keywordfont do}}
\algrenewcommand\algorithmicuntil{{\keywordfont until}}
\algrenewcommand\algorithmicfunction{{\keywordfont function}}
\algrenewcommand\algorithmicif{{\keywordfont if}}
\algrenewcommand\algorithmicthen{{\keywordfont then}}
\algrenewcommand\algorithmicelse{{\keywordfont else}}
\algrenewcommand\algorithmicreturn{{\keywordfont return}}

\renewcommand\thealgorithm{}
\newcommand{\setalglineno}[1]{%
  \setcounter{ALC@line}{\numexpr#1-1}}

\newcommand{\sub}[1]{\textsubscript{#1}}
\renewcommand{\tt}[1]{\texttt{#1}}
\renewcommand{\sl}[1]{\textsl{#1}}
\renewcommand{\it}[1]{\textit{#1}}
\renewcommand{\sc}[1]{\textsc{#1}}
\renewcommand{\bf}[1]{\textbf{#1}}
\newcommand{\nf}[1]{{\normalfont{\texttt{#1}}}}
\newcommand{\cmt}[1]{\Comment{#1}}
\newcommand{\head}{head}
\newcommand{\size}{size }

\usepackage{amsmath,amssymb,amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{observation}[theorem]{Observation}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{invariant}[theorem]{Invariant}


\begin{document}
\title{Wait-free Queues with Polylogarithmic Step Complexity}
\author{Hossein Naderibeni \\supervised by Eric Ruppert}
\maketitle

\begin{abstract}
In this work, we are going to introduce a novel lock-free queue implementation.
Linearizability and lock-freedom are standard requirements for designing shared data structures. All existing linearizable, lock-free queues in the literature have a common problem in their worst case called CAS Retry Problem. Our contribution is solving this problem while outperforming the previous algorithms.
\end{abstract}

\tableofcontents
\pagebreak
\section{Introduction}
Shared data structures have become an essential field in distributed algorithms research.
We are reaching the physical limits of how many transistors we can place on a CPU core. The industry solution to provide more computational power is to increase the number of cores of the CPU. This is why distributed algorithms have become important. It is not hard to see why multiple processes cannot update sequential data structures designed for one process. For example, consider two processes trying to insert some values into a sequential linked list simultaneously. Processes $p,q$ read the same tail node, $p$ changes the next pointer of the tail node to its new node and after that $q$ does the same. In this run, $p$'s update is overwritten. One solution is to use locks; whenever a process wants to do an update or query on a data structure, the process locks it, and others cannot use it until the lock is released. Using locks has some disadvantages; for example, one process might be slow, and holding a lock for a long time prevents other processes from progressing. Moreover, locks do not allow complete parallelism since only the one process holding the lock can make progress. 


The question that may arise is, ``What properties matter for a lock-free data structure?'', since executions on a shared data structure are different from sequential ones, the correctness conditions also differ. To prove a concurrent object works perfectly, we have to show it satisfies safety and progress conditions. A \textit{safety condition} tells us that the data structure does not return wrong responses, and a \textit{progress property} requires that operations eventually terminate.


The standard safety condition is called \textit{linearizability}, which ensures that for any concurrent execution on a linearizable object, each operation should appear to take effect instantaneously at some moment between its invocation and response. Figure \ref{fig::goodexample} is an example of an execution on a linearizable  queue that is initially empty. The arrow shows time, and each rectangle shows the time between the invocation and the termination of an operation. Since \texttt{Enqueue(A)} and \texttt{Enqueue(B)} are concurrent, \texttt{Enqueue(B)} may or may not take effect before \texttt{Enqueue(A)}. The execution in Figure \ref{fig::badexample} is not linearizable since \texttt{A} has been enqueued before \texttt{B}, so it has to be dequeued first.

\begin{figure}[hbt]
  \center\includegraphics[scale=0.5]{pics/good}
  \caption{\label{fig::goodexample}An example of a linearizable execution. Either \texttt{Enqueue(A)} or \texttt{Enqueue(B)} could take effect first since they are concurrent.}
\end{figure}

\begin{figure}[hbt]
  \center\includegraphics[scale=0.5]{pics/bad}
  \caption{\label{fig::badexample}An example of an execution that is not linearizable. Since \texttt{Enqueue(A)} has completed before \texttt{Enqueue(B)} is invoked the \texttt{Dequeue()} should return \texttt{A} or nothing.}
\end{figure}


There are various progress properties; the strongest is wait-freedom, and the more common is lock-freedom. An algorithm is \textit{wait-free} if each operation terminates after a finite number of its own steps. We call an algorithm \textit{lock-free} if, after a sufficient number of steps, one operation terminates. A wait-free algorithm is also lock-free but not vice versa; in an infinite run of a lock-free algorithm there might be an operation that takes infinitely many steps but never terminates.

In section 2 we talk about previous queues and their common problems. We also talk about polylogarithmic construction of shared objects.

Jayanti~\cite{DBLP:conf/podc/Jayanti98a} proved an $\Omega(\log p)$ lower bound on the worst-case shared-access time complexity of $p$-process universal constructions. He also introduced~\cite{DBLP:conf/podc/ChandraJT98} a construction that achieves $\textsc{O}(\log^2 p)$ shared accesses. Here, we first introduce a universal construction using $\textsc{O}(\log p)$ CAS operations~\cite{DBLP:conf/fsttcs/JayantiP05}. In section 3 we introduce a polylogarithmic step wait-free universal construction. Our main ideas in of the universal construction also appear in our Queue Algorithm (\ref{algQ}). The main short come of our universal construction is using big CAS objects. We use the universal construction as a stepping stone towards our queue algorithm, so we will not explain it in too much detail.

In section 4 we introduce a concurrent wait-free datastructure, to agree on the order of the operations invoked on some processes.

In section 5 we introduce our main work, the queue; prove its linearizability and wait-freeness.

\pagebreak
\section{Related Work}
\subsection{List-based Queues}
In the following paragraphs, we look at previous lock-free queues.
Michael and Scott~\cite{DBLP:conf/podc/MichaelS96} introduced a lock-free queue which we refer to as the MS-queue. A version of it is included in the standard Java Concurrency Package. Their idea is to store the queue elements in a singly-linked list (see Figure~\ref{fig::msq}). Head points to the first node in the linked list that has not been dequeued, and Tail points to the last element in the queue. To insert a node into the linked list, they use atomic primitive operations like \texttt{LL/SC} or \texttt{CAS}. If $p$ processes try to enqueue simultaneously, only one can succeed, and the others have to retry. This makes the amortized number of steps to be $\Omega(p)$ per enqueue. Similarly, dequeue can take $\Omega(p)$ steps.

\begin{figure}[hbt]
  \center\includegraphics[scale=0.4]{pics/msqueue}
  \caption{\label{fig::msq}MS-queue structure, enqueue and dequeue operations. In the first diagram the first element has been dequeued. Red arrows show new pointers and gray dashed arrows show the old pointers.}
\end{figure}


Moir, Nussbaum, and Shalev~\cite{DBLP:conf/spaa/MoirNSS05} presented a more sophisticated queue by using the elimination technique. The elimination mechanism has the dual purpose of allowing operations to complete in parallel and reducing contention for the queue. An Elimination Queue consists of an MS-queue augmented with an elimination array. Elimination works by allowing opposing pairs of concurrent operations such as an enqueue and a dequeue to exchange values when the queue is empty or when concurrent operations can be linearized to empty the queue. Their algorithm makes it possible for long-running operations to eliminate an opposing operation. The empirical evaluation showed the throughput of their work is better than the MS-queue, but the worst case is still the same; in case there are $p$ concurrent enqueues, their algorithm is not better than MS-queue. 

Hoffman, Shalev, and Shavit~\cite{DBLP:conf/opodis/HoffmanSS07} tried to make the MS-queue more parallel by introducing the Baskets Queue. Their idea is to allow more parallelism by treating the simultaneous enqueue operations as a basket. Each basket has a time interval in which all its nodes' enqueue operations overlap. Since the operations in a basket are concurrent, we can order them in any way. Enqueues in a basket try to find their order in the basket one by one by using \texttt{CAS} operations. However, like the previous algorithms, if there are still $p$ concurrent enqueue operations in a basket, the amortized step complexity remains $\Omega(p)$ per operation.

\begin{figure}[hbt]
  \center\includegraphics[scale=0.3]{pics/baskets}
  \caption{Baskets queue idea. There is a time that all operations in a basket were running concurrently, but only one has succeeded to do \texttt{CAS}. To order the operations in a basket, the mechanism in the algorithm for processes is to \texttt{CAS} again. The successful process will be the next one in the basket and so on.}
\end{figure}

Ladan-Mozes and Shavit~\cite{DBLP:journals/dc/Ladan-MozesS08} presented an Optimistic Approach to Lock-Free FIFO Queues. They use a doubly-linked list and do fewer \texttt{CAS} operations than MS-queue. But as before, the worst case is when there are $p$ concurrent enqueues which have to be enqueued one by one. The amortized worst-case complexity is still $\Omega(p)$ \texttt{CAS}es.

Hendler et al.~\cite{DBLP:conf/spaa/HendlerIST10} proposed a new paradigm called flat combining. Their queue is linearizable but not lock-free. Their main idea is that with knowledge of all the history of operations, it might be possible to answer queries faster than doing them one by one. In our work we also maintain the whole history. They present experiments that show their algorithm performs well in some situations.

Gidenstam, Sundell, and Tsigas~\cite{DBLP:conf/opodis/GidenstamST10} introduced a new algorithm using a linked list of arrays. Global head and tail pointers point to arrays containing the first and last elements in the queue. Global pointers are up to date, but head and tail pointers may be behind in time. An enqueue or a dequeue searches in the head array or tail array to find the first unmarked element or last written element (see~Figure~\ref{fig::sundell}). Their data structure is lock-free. Still, if the head array is empty and $p$ processes try to enqueue simultaneously, the step complexity remains $\Omega(p)$.

\begin{figure}[hbt]
  \center\includegraphics[scale=0.5]{pics/sundell}
    \caption{\label{fig::sundell}Global pointers point to arrays. Head and Tail elements are blue, dequeued elements are red and current elements of the queue are green.}
\end{figure}

Kogan and Petrank~\cite{DBLP:conf/ppopp/KoganP11} introduced wait-free queues based on the MS-queue and use Herlihy's helping technique to achieve wait-freedom. Their step complexity is $\Omega(p)$ because of the helping mechanism.

%todo: comparison
%Milman et al.~\cite{DBLP:conf/spaa/MilmanKLLP18} designed BQ: A Lock-Free Queue with Batching. Their idea of batching allows a sequence of operations to be submitted as a batch for later execution. It supports a new notion introduced by the authors called Extended Medium Futures Linearizability.

%Nikolaev and Ravindran~\cite{DBLP:journals/corr/abs-2201-02179} wCQ to be completed.
In the worst-case step complexity of all the list-based queues discussed above, there is a $p$ term that comes from the case all $p$ processes try to do an enqueue simultaneously. Morrison and Afek call this the \textit{CAS retry problem}~\cite{DBLP:conf/ppopp/MorrisonA13}. It is not limited to list-based queues and array-based queues share the CAS retry problem as well~\cite{DBLP:conf/spaa/TsigasZ01,DBLP:conf/icdcn/Shafiei09,DBLP:conf/iceccs/ColvinG05} . We are focusing on seeing if we can implement a queue in sublinear steps in terms of $p$ or not.

\subsection{Universal Constructions}
Herlihy discussed the possibility of implementing shared objects from other objects~\cite{10.1145/114005.102808}. A \textit{universal construction} is an algorithm that can implement a shared version of any given sequential object. We can implement a concurrent queue using a universal construction. Jayanti proved an $\Omega(\log p)$ lower bound on the worst-case shared-access time complexity of $p$-process universal constructions~\cite{DBLP:conf/podc/Jayanti98a}. He also introduced a construction that achieves $\textsc{O}(\log^2 p)$ shared accesses~\cite{DBLP:conf/podc/ChandraJT98}. His universal construction can be used to create any data structure, but its implementation is not practical because of using unreasonably large-sized \texttt{CAS} operations.

Ellen and Woelfel introduced an implementation of a Fetch\&Inc object with step complexity of $O(\log p)$ using $O(\log n$)-bit \texttt{LL/SC} objects, where $n$ is the number of operations~\cite{10.1007/978-3-642-41527-2_20}. Their idea has similarities to Jayanti's construction, and they represent the value of the Fetch\&Inc using the history of successful operations. 

%FIFO queues have a wide range of use in OS and applications. The current state-of-the-art queues are implemented using linked lists; that's why they have factor p in their time complexity.

%\href{https://docs.google.com/spreadsheets/d/1cL1tgXXdljkh462sMwkTVMHH_k0MBTmslIVM3xA5VS4/edit#gid=0}{Table of previous works}

%There is a connection between queues and universal constructions. We can implement a universal construction using a queue. We can store operations in the shared queue and compute operations' responses using the queue's content. Some impractical universal constructions are using big words as "...".

\subsection{Attiya Fourier Lower Bound}
\pagebreak

\pagebreak
\section{Our Queue}
Jayanti and Petrovic introduced a wait-free polylogarithmic multi-enqueuer single-dequeuer queue~\cite{DBLP:conf/fsttcs/JayantiP05}. We benefit from some ideas of their work to design a a polylogarithmic multi-enqueuer multi-dequeuer queue. Our algorithm despite them does not use \texttt{CAS} operations with big words and does not put a limit on the number of concurrent operations. I our model there are $p$ procesess doing \nf{Enqueue(),Dequeue()} operations concurrently. We use a shared tree among the processes (see Figure~\ref{fig::blocktree}) to agree on one total ordering on the operations invoked by processes. Each process has a leaf which the order of operations invoked by the process is stored in it. When a process wishes to do an operation it appends the operation to its leaf and then tries to propagate its new operation up to the tree's root. In each node the ordering of operations propagated up to it is stored. All processes agree on the sequence stored in the root and it is defined to be the linearization ordering. 
\begin{figure}[hbt]
\begin{center}
\Tree [ [ [ $P_1$ $P_2$ ] [ $P_3$ $P_4$ ] ]
          [ [ $P_5$ $P_6$ ] [ $P_7$ $P_8$ ] ] ]
\end{center}
\caption{Ù‘\label{fig::blocktree}Each process has a leaf and in each node there is an ordering of operations stored. Each node tries to propagate its operations up to the root, which stores the total ordering of all operations.}  
\end{figure}

\it{Add sequence to nodes}

We could implement the sequence stored in each node using an array of the queue operations and append some operations to the sequence by doing \tt{k-CAS} operation on the end of the array. To do a propagate step on node $n$ in the tree, we aggregate the operations from node $n$'s both children (that have not already been propagated to $n$) and try to append them into $n$. We call this procedure \sc{Refresh}$(n)$. The main idea is that if we call \sc{Refresh}$(n)$ twice, the operations in $n$'s children before the first \sc{Refresh}$(n)$ are guaranteed to be in $n$.
Because if both of the \sc{Refresh}$(n)$s fail to do \nf{k-CAS} then there is another instance of \nf{Refresh()} in between which has succeeded to do \nf{CAS} and has already appended the operations that the first \nf{Refresh} was trying to append. This mechanism makes us overcome the \nf{CAS} Retry Problem.

\begin{figure}[hbt]
\begin{center}
\begin{subfigure}[b]{.49\textwidth}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tikzpicture}[level 1/.style={level distance=1.4cm,sibling distance=0.5cm}]
\Tree [.{\begin{tabular}{|c|c|c|c|c|c|}  \hline $r_1$ & $l_1$ & $r_2$ & $l_2$ & $l_3$ \\ \hline \end{tabular}}
{\begin{tabular}{|c|c|c||c|c|}  \hline $l_1$ & $l_2$ & $l_3$ & $l_4$ & $l_5$ \\ \hline \end{tabular}}
{\begin{tabular}{|c|c||c|c|}  \hline $r_1$ & $r_2$ & $r_3$ & $r_4$\\ \hline \end{tabular}} ]
\end{tikzpicture}}
  \caption{Operations after $||$ are new.}
\end{subfigure}
\hfill
\begin{subfigure}[b]{.49\textwidth}
  \centering
  \resizebox{\columnwidth}{!}{
  \begin{tikzpicture}[level 1/.style={level distance=1.4cm,sibling distance=0.5cm}]
\Tree [.{\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}  \hline $r_1$ & $l_1$ & $r_2$ & $l_2$ & $l_3$ & $l_4$ & $l_5$ & $r_3$ & $r_4$  \\ \hline \end{tabular}}
{\begin{tabular}{|c|c|c|c|c||}  \hline $l_1$ & $l_2$ & $l_3$ & $l_4$ & $l_5$ \\ \hline \end{tabular}}
{\begin{tabular}{|c|c|c|c||}  \hline$r_1$ & $r_2$ & $r_3$ & $r_4$\\ \hline \end{tabular}} ]
\end{tikzpicture}}
  \caption{New operations are added to the parent node.}
\end{subfigure}
\caption{\label{fig::propagstep} Before and after of a \sc{Refresh}$(n)$ with successful \nf{CAS}. Operations propagating from the left child are numbered with $l$ and from the right child by $r$ and the operations in children after $||$ are new.}
\end{center}
\end{figure}
\it{fix $||$}
\begin{figure}[h]
\begin{center}
\begin{tikzpicture}
  
\Tree [.{$\big\{op_2^1,op_2^2,op_3^1\big\},\big\{op_4^1$, $op_3^2\big\},\big\{op_1^1, op_4^2\big\},\big\{op_1^2\big\}...$ }  [.{ $\big\{op_2^1, op_2^2\big\},\big\{op_1^1\big\},\big\{op_1^2\big\}...$ }
      {\begin{tabular}{|l|c|c|c}  \hline $op_1^1$ & $op_1^2$ & ... \\ \hline\end{tabular}} {\begin{tabular}{|l|c|c|c}  \hline $op_2^1$ & $op_2^2$ & ... \\ \hline\end{tabular}} ] [.{ $\big\{op_3^1\big\},\big\{op_4^1$, $op_3^2\big\},\big\{op_4^2\big\},...$ } {\begin{tabular}{|l|c|c|c}  \hline $op_3^1$ & $op_3^2$ & ... \\ \hline\end{tabular}} {\begin{tabular}{|l|c|c|c} \hline $op_4^1$ & $op_4^2$ & ... \\ \hline\end{tabular}} ] ]

\end{tikzpicture}
\caption{\label{fig::set} In each internal node, we store the set of all the operations propagated together, and one can arbitrarily linearize the sets of concurrent operations among themselves. Since we linearize operations when they are added to the root, ordering the blocks in the root is important.}
\end{center}
\end{figure}

The solution for  implementing the orderings in the tree told above is not efficient, because there are big \nf{CAS}es and operations information are copied all the way up to the root.
Instead of storing operations explicitly in the nodes, we can keep track of some statistics of them. This allows us to \texttt{CAS} fixed-size objects in each \textsc{Refresh}($n$). To do that, we introduce blocks that only contain the number of operations from the left and the right child in a \texttt{Refresh}() procedure and only propagate the statistics block of the new operations. In each \nf{Refresh()} there is at most one operation from each process trying to be propagated, because one operation cannot invoke two operations concurrently. Furthermore since the operations in a \textsc{Refresh()} step are concurrent we can linearize them  among themselves in any order we wish. Note that if two operations are in read one \textsc{Refresh()} step in a node they are going to be propagated up to the root together. Our choice is to put the operations propagated from the left child before the operations propagated from the right child. In this way if we know the number of operations from the left child and the number of operations from the right child in a block  we have a complete ordering on the operations.

A process may wish to know the $i$th propagated operation or the rank of a propagated operation in the linearization. In our case of implementing a queue, we can make an assumption that one process only wishes to know the rank of a dequeue and one tries to get an enqueue with its rank. \nf{enqueue}s and \nf{dequeue}s are appended to the tree and when we want to find the response to a \nf{dequeue}, we compute the place of the dequeue in the linearization and using the rank of the dequeue among deqeueues and some information stored in the root we compute which enqueue is the answer to the dequeue or if the answer is null. If the answer was some enqueue we find the enqueue using \texttt{DSearch(i)} and \nf{GetENQ(n,b,i)}. \texttt{DSearch(i)} finds the block containing the $i$th enqueue in the root and \nf{GetENQ(n,b,i)} finds its sub-block recursively to reach a leaf. \texttt{Index()} is similar but more complicated, finding super-blocks from a leaf to the root. The main challenge in each level of \texttt{Get(i)} and \texttt{Index(op)} is that it should take polylogarithmic steps with respect to $p$. After appending operation \texttt{op} to the root, processes can find out information about the linearization ordering using \texttt{Get(i)} and \texttt{Index(op)}. Each block stores an extra constant amount of information (like prefix sums) to allow binary searches to find the required block in a node quickly.

\paragraph{Implementing Queue using Block Tree}
In this work, we design a queue with $O(\log^2 p +\log n)$ steps per operation, where $n$ is the number of total operations invoked. We avoid the $\Omega(p)$ worst-case step complexity of existing shared queues based on linked lists or arrays (CAS Retry Problem). A queue stores a sequence of elements and supports two operations, enqueue and dequeue. \texttt{Enqueue(e)} appends element \texttt{e} to the sequence stored.\texttt{Dequeue()} removes and returns the first element among in the sequence. If the queue is empty it returns \texttt{null}. Knowing index $i$ is the tail of the queue, we can return the dequeue response using \texttt{Get(i)}.  So in the rest we modify block tree to compute \texttt{i} for each \texttt{Dequeue()} to achieve a FIFO queue.

\begin{figure}

\begin{center}
\begin{tikzpicture}[level 1/.style={level distance=3.3cm,sibling distance=1cm},
	level 2/.style={level distance=2.5cm,sibling distance=0.5cm},
	level 3/.style={level distance=1.8cm,sibling distance=1.2cm}]
  

\Tree [.{\begin{tabular}{||c|c|c||}  \hline 0,11 & 12,6 & 8,36 \\ \hline\end{tabular}}
 [.{\begin{tabular}{||c||c||}  \hline 10,2 & 5,3 \\  \hline \multicolumn{1}{c||}{2} & \multicolumn{1}{c}{3} \end{tabular}}
 [.{\begin{tabular}{||c|c||c||}\hline 2,3 & 4,1 & 0,5 \\\hline\multicolumn{2}{c||}{1} & \multicolumn{1}{c}{2}\end{tabular}} $\vdots$ $\vdots$ ]
  [.{\begin{tabular}{||c||c||}  \hline 0,2 & 1,2 \\  \hline \multicolumn{1}{c||}{1} & \multicolumn{1}{c}{2} \end{tabular}} $\vdots$ $\vdots$ ] ]
          [.{\begin{tabular}{||c||c||c|c||}  \hline 3,8 & 6,0 & 14,6 & 0,16 \\ \hline \multicolumn{1}{c||}{1} & \multicolumn{1}{c||}{2} & \multicolumn{2}{c}{3}\end{tabular}}
           [.{\begin{tabular}{||c||c||c|c||}  \hline 0,3 & 4,2 & 5,1 & 3,5 \\ \hline \multicolumn{1}{c||}{1} & \multicolumn{1}{c||}{2}& \multicolumn{2}{c}{3}\end{tabular}} $\vdots$ $\vdots$ ]
            [.{\begin{tabular}{||c|c||c||c|c||}  \hline 2,1 & 5,0 & 4,2 & 3,4 & 6,3 \\ \hline \multicolumn{2}{c||}{1} & \multicolumn{1}{c||}{3} & \multicolumn{2}{c}{4}\end{tabular}} $\vdots$ $\vdots$ ] ] ]

\end{tikzpicture}
\end{center}
\caption{\label{fig:block} Showing concurrent operation sets with blocks. Each block consists of a pair(left, right) indicating the number of operations from the left and the right child, respectively. Block (12,6) in the root contains blocks (10,2) from the left child and (6,0) from the right child. Blocks between two lines $||$ are propagated together to the parent. For example, Blocks (2,3) and (4,1) from the leftmost leaf and (0,2) from its sibling are propagated together into the block (10,2) in their parent. The number underneath a group of blocks in a node indicates which block in the node's parent those blocks were propagated to. Each block $b$ in node $n$ is the aggregation of blocks in the  children of $n$ that are newly read by the\textsc{Propagate}() step that created block $b$. For example, the third block in the root (8,36) is created by merging block (5,3) from the left child and (14,6) and (0,16) from the right child. Block (5,3) also points to elements from blocks (0,5) and (1,2). We choose to linearize operations in a block from the left child before those from the right child as a convention. Operations within a block of the root can be ordered in any way that is convenient. In effect, this means that if there are concurrent new blocks in a \textsc{Refresh}() step from several processes we linearize them in the order of their process ids. So for example  operations aggregated in block (10,2) are in the order (2,3),(4,1),(0,2). All blocks from the left child with come before the right child and the order of blocks of each child is preserved among themselves.
}
\end{figure}

\paragraph{}
\textsc{GetIndex}($i$) returns the $i$th operation stored in the block tree sequence. We do that by finding the block $b_i$ containing $i$th element in the root, and then recursively finding the subblock of $b_i$ which contains $i$th element. To make this recursive search faster, instead of iterating over all elements in sequence of blocks we store prefix sum of number of elements in the blocks sequence and pointers to make BSearch faster.

Furthermore, in each block, we store the prefix sum of left and right elements. Moreover, for each block, we store two pointers to the last left and right subblock of it (see fig \ref{fig::pointer} and \ref{fig:prefix}).

\begin{figure}
\begin{center}
\begin{tikzpicture}[level 1/.style={level distance=2.3cm,sibling distance=1cm}]
  
\Tree [.{\begin{tabular}{||c||c||c|c||}  \hline 3,8 & 6,0 & 14,6 & 0,16 \\ \hline  0,0 & 3,8 & 9,8 & 23,14\end{tabular}}
           [.{\begin{tabular}{||c||c||c|c||}  \hline 0,3 & 4,2 & 5,1 & 3,5 \\ \hline 0,0 & 0,3 & 4,5 & 9,6 \end{tabular}} ]
            [.{\begin{tabular}{||c|c||c||c|c||}  \hline 2,1 & 5,0 & 4,2 & 3,4 & 6,3 \\ \hline 0,0 & 2,1 & 7,1 & 11,3 & 14,7 \end{tabular}} ] ]


\end{tikzpicture}
\end{center}
\caption{\label{fig:prefix} Using Prefix sums in blocks. When we want to find block b elements in its children, we can use binary search. The number below each block shows the count of elements in the previous blocks.}
\end{figure}

\begin{figure}[hbt]
\centering
  \includegraphics[width=5in]{pics/pointers}
  \caption{Block have pointers to the starting block of theirs for each child. \label{fig::pointer}}
\end{figure}


\paragraph{}
Starting from the root, \textsc{GetIndex}($i$) BSearches $i$ in the prefix sum array to find block containing $i$th operation, then continues recursively calling \textsc{GetElement}($b,i$) to find $i$th element of block $b$. From lemma $\ref{lem:block_size}$ we know a block size is at most $p$. So BSearch takes at most \textsc(O)$(\log p)$, since  with knowing pointers of a block and its previous block we can determine the base \texttt{(domain ?)} to search and its size is \textsc{O}$(p)$.



\paragraph{\texttt{CreateBlock()}} \texttt{CreateBlock(n)} returns a block containing new operations of \texttt{n}'s children. \texttt{b\textsuperscript{$\prime$}.end\textsubscript{left}} stores the index of the rightmost subblock of left child of \texttt{b}'s previous block. Other attributes are assigned values followed by definition.
\begin{figure}[hbt]
  \center\includegraphics[width=5.5in]{pics/createblock}
  \caption{\label{fig::createBlock}Snapshot of a CreateBlock()}
\end{figure}

\begin{figure}[hbt]
  \center\includegraphics[width=4in]{pics/doublerefresh.png}
  \caption{The second failed Refresh is assuredly concurrent to a Successful \texttt{Refresh()} with \texttt{CreateBlock} line after first failed \texttt{Refresh}'s \texttt{CreateBlock()}.}
\end{figure}

\paragraph{Computing \texttt{Get(n, b, i)}}

\paragraph{How \texttt{Refresh(n)} works.}

\paragraph{Computing superblock}


%\subsection{What should a \texttt{Dequeue()} return, regarding history of operations?}
\paragraph{}
Now, we describe how to use the tree to implement a queue. Consider the following execution of operations. \texttt{Enqueue(e)} appends an operation with input argument \texttt{e} in the block tree. What should a \texttt{Dequeue()} return? To compute the response of a \texttt{Dequeue()}, process $p$ first appends a \texttt{DEQ} operation to the tree. Then $p$ finds the rank of the \texttt{DEQ} using \texttt{Index()}, the rank of the \texttt{DEQ} and the information stored in the root about the queue $p$ computes the rank of  the \texttt{ENQ} having the answer of the \texttt{DEQ}. Finally $p$ returns the argument of that \texttt{ENQ} using \texttt{Get(i)}.

\begin{table}[hbt]
\centering
  \begin{tabular}{c|c|c|c|c|c|c|c|c|c}
    \hline \texttt{ENQ(5)}& \texttt{ENQ(2)}& \texttt{DEQ()}& \texttt{ENQ(3)}&\texttt{DEQ()}& \texttt{DEQ()}& \texttt{DEQ()}& \texttt{ENQ(4)}& \texttt{ENQ(6)}& \texttt{DEQ()}\\ \hline
  \end{tabular}
  \caption{An example histoy of operations on the queue}
\end{table}



\paragraph{}
A non-null dequeue is one that returns a non-null value. In the example above, \texttt{Dequeue()} operations return \texttt{5, 2, 3, null, 4} in order. Before \texttt{ENQ(4)} the queue gets empty so the last \texttt{DEQ()} returns null. If the queue is non-empty and $r$ \texttt{Dequeue()} operations have returned a non-null response, then $i$th \texttt{Dequeue()} returns the input of the $r+1$th \texttt{Enqueue()}. So, in order to answer a Dequeue, it's sufficent to know the size of the queue and the number of previous non-null dequeues.

%\texttt{DEQ[i] = (size>0) ? ENQ[r+1] : null;}


%\subsection{What should a \texttt{Dequeue()} return, regarding history of blocks of operations?}

\paragraph{}
In the Block Tree, we did not store the sequence of operations explicitly but instead stored blocks of concurrent operations to optimize \texttt{Propagate()} steps and increase parallelism. So now the problem is to find the result of each Dequeue. From lemma \ref{lem:block_size} we know we can linearize operations in a block in any order; here, we choose to decide to put Enqueue operations in a block before Dequeue operations. In the next example, operations in a cell are concurrent. \texttt{DEQ()} operations return \texttt{null, 5, 2, 1, 3, 4, null} respectively. We will next describe how these values can be computed efficiently.

\begin{table}[hbt]
\centering
  \begin{tabular}{c|c|c|c}
    \hline \texttt{DEQ()} & \texttt{ENQ(5)}, \texttt{ENQ(2)}, \texttt{ENQ(1)}, \texttt{DEQ()}& \texttt{ENQ(3)}, \texttt{DEQ()}&  \texttt{ENQ(4)}, \texttt{DEQ()}, \texttt{DEQ()}, \texttt{DEQ()}, \texttt{DEQ()}\\ \hline
  \end{tabular}
  \caption{An example history of operation blocks on the queue}
\end{table}


\paragraph{}
Now, we claimed that by knowing the current size of the queue and the number of non-null dequeue operations before the current dequeue, we could compute the index of the resulting \texttt{Enqueue()}. We apply this approach to blocks; if we store the size of the queue after each block of operations happens and the number of non-null dequeues dequeues till a block, we can compute each dequeue's index of result in \textsc{O}$(1)$ steps.

\begin{table}[hbt]
\centering
  \begin{tabular}{c|c|c|c|c}
    \hline &\texttt{DEQ()} & \texttt{ENQ(5)}, \texttt{ENQ(2)}, \texttt{ENQ(1)}, \texttt{DEQ()}& \texttt{ENQ(3)}, \texttt{DEQ()}&  \texttt{ENQ(4)}, \texttt{DEQ()}, \texttt{DEQ()}, \texttt{DEQ()}, \texttt{DEQ()}\\ \hline
    \#enqueues & 0 & 3 & 1 & 1 \\ \hline
        \#dequeues & 1 & 1 & 1 & 4 \\ \hline
            \#non-null dequeues & 0 & 1 & 2 & 5 \\ \hline
                size & 0 & 2 & 2 & 0 \\ \hline
  \end{tabular}
  \caption{Augmented history of operation blocks on the queue}
\end{table}

%\begin{definition}
%  \texttt{index(op)}: index of the given Dequene among same type operation in conataing block.
%  
%  \texttt{INDEX(op)}: index of the given Dequene among same type operation in all operations.
%  
%\end{definition}

Size and the number of non-null dequeues for $b$th block could be computed this way:\\
\texttt{size[b]= max(size[b-1] +enqueues[b] -dequeues[b], 0)}\\
\texttt{non-null dequeues[b]= non-null dequeues[b-1] +dequeues[b] -size[b-1] -enqueues[b]}

Given \texttt{DEQ} is in block \texttt{b}, \texttt{response(DEQ)} would be:\\
\texttt{(size[b-1]- index of DEQ in the block's dequeus >=0) ? ENQ[non-null dequeus[b-1]+ index of DEQ in the block's dequeus] : null;}


\begin{figure}[hbt]
\centering
  \includegraphics[width=5in]{pics/queue}
  \caption{Fields stored in the Queue nodes. \label{fig::queue}}
\end{figure}


\subsection{Pseudocode description}

\paragraph{Specification}
A Queue is a shared data structure that stores a sequence of elements. It has two methods \texttt{Enqueue(e)} and \texttt{Dequeue()}. \texttt{Enqueue(e)} adds \texttt{e} to the end of the sequence. \texttt{Dequeue()} returns the first element stored in the sequence and removes it from the sequence.

\paragraph{Tree}
In order to reach an agreement on the order of operations among $p$ processes, we use a Tournament Tree. Leaf \texttt{l\textsubscript{i}} is assigned to a process \texttt{i}. Each process adds $op$ to its leaf. In each internal node an ordering of operations in its subtree is stored. All processes agree on the total ordering of all operations stored in the root. This ordering will be the linearization of the operations.
\paragraph{Implicit Storing Blocks}
For efficiency, instead of storing explicit sequence of operations in nodes of the Tournament Tree, we use Blocks. A Block is a constant size object that implicitly represents a sequence of operations. In each node there is an array of Blocks.

Block $b$ contains subblocks in the left and right children. WLOG left subblocks of $b$ are some consecutive blocks in the left child starting from where previous block of $b$ has ended to the the end of $b$. See Figure \ref{fig::createBlock} .

We store ordering among \texttt{operation}s in the tournament tree constructed by \texttt{node}s. In each \texttt{node} we store pointers to its relatives, an array of \texttt{block}s and an index to the first empty \texttt{block}. Furthermore in \texttt{leaf} nodes there is an array of \texttt{operations} where each \texttt{operation} is stored in one cell with the same index in \texttt{blocks}. There is a \texttt{counter} in each \texttt{node} incrementing after a successful \texttt{Refresh()} step. It means after that some bunch of \texttt{block}s in a node have propagated into the parent then the \texttt{counter} increases. Each new \texttt{block} added to a node sets its \texttt{time} regarding \texttt{counter}. This helps us to know which blocks have aggregated together to a block, not precisely though. We also store the index of the aggregated \texttt{block} of a \texttt{block} with \texttt{time} $i$ in \texttt{super[i]}. 

In each \texttt{block} we store 4 essential stats that implicitly summarize which operations are in the block \texttt{num\textsubscript{enq-left}}, \texttt{num\textsubscript{deq-left}}, \texttt{num\textsubscript{enq-right}}, \texttt{num\textsubscript{deq-right}}. In order to make \texttt{BSearch()}es faster we store prefix sums as well and there are some more general stats that help to make pseudocode more readable but not necessary.

To compute the head of the \texttt{queue} before a \texttt{dequeue} two more fields are stored in the root \texttt{size} and \texttt{sum\textsubscript{non-null deq}}. \texttt{size} in a \texttt{block} shows the number of elements after the \texttt{block} has finished and \texttt{sum\textsubscript{non-null deq}} is the total number of non-null dequeues till the \texttt{block}.

\texttt{Enqueue(e)} just \texttt{append}s an \texttt{operation} with \texttt{element e} to the \texttt{root}. \texttt{Dequeue()} appends an \texttt{operation} to the root and computes its ordering and the \texttt{enqueue operation} containing the head before it calling \texttt{ComputeHead()} and then \texttt{gets} and returns the \texttt{operation}'s element.

\texttt{Append(op)} adds \texttt{op} to the invoking process's leaf's \texttt{ops} and \texttt{blocks}, \texttt{propagate}s it up to the root and if the \texttt{op} is a dequeue returns its order in residing block in the root and the block's index. As we said later \texttt{Propagate()} assuredly aggregates new blocks to a block in the parent by calling \texttt{Refresh()} two times. \texttt{Refresh(n)} creates a block, tries to CAS it into the p\texttt{n}'s \texttt{blocks} and if it was successful updates \texttt{super} and \texttt{counter} in both of \texttt{n}'s children.

We only want to know the \texttt{element} of \texttt{enqueue} operations and compute ordering for \texttt{dequeue} operations. That's the reason here \texttt{Get()} searches between enqueues only and \texttt{Index()} returns ordering of a dequeue among dequeues. \texttt{Get(n, b ,i)} decides the requested element is in which child of n and continues to search recursively. \texttt{index(n, i, b)} calculates the ordering of the given operation in \texttt{n}'s parent each step and finally returns the result among total ordering.



\pagebreak
\subsection{Pseudocode}

\begin{algorithm}
\caption{Tree Fields Description}
\begin{algorithmic}[1]
\setcounter{ALG@line}{100}
\begin{multicols}{2}

%\Statex \bf{Structure}

\Statex $\diamondsuit$ \tt{\sl{Shared}}
\begin{itemize}
\item \textsf{A binary tree of \tt{Node}s with one \tt{leaf} for each process. \tt{root} is the root \nf{node}.}
%such that \tt{root} is the root and the left child and the right child and the parent of \tt{Tree[i]} are \tt{Tree[2i+1]}, \tt{Tree[2i+2]} and \tt{Tree[i/2]}.
\end{itemize}

\Statex $\diamondsuit$ \tt{\sl{Local}}
\begin{itemize}
\item \tt{\sl{Node} leaf:} \sf{ process's leaf in the tree.}
\end{itemize}


%\Statex $\diamondsuit$ \tt{\sl{Structures}}

\Statex $\blacktriangleright$ \tt{\sl{Node}}
\begin{itemize}
\item \tt{\sl{*Node} left, right, parent} \textsf{: initialized  when creating the tree.}
\item \tt{\sl{BlockList}}
\item \tt{\sl{int} \head= 1}\textsf{: \#\tt{block}s in \tt{blocks}. \tt{blocks[0]} is a block with all integer fields equal to zero.}
%\item \tt{\sl{int} num\sub{propagated}= 0}\textsf{} \textsf{: \# groups of blocks that have been propagated from the node to its parent.}
\end{itemize}

%\Statex $\blacktriangleright$ \tt{\sl{Root} extends \sl{Node}}
%\begin{itemize}
%  
%\item \tt{\sl{PBRT} blocks}
%
%  \textsf{\tt{BlockList} is implemented with a persistent red-black tree.}
%  
%\end{itemize}

%\Statex $\blacktriangleright$ \tt{\sl{NonRootNode} extends \sl{Node}}
%\begin{itemize}
    
%\end{itemize}

%\Statex $\blacktriangleright$ \tt{\sl{Leaf} extends \sl{Node}}
%\begin{itemize}
%  
%  \item \tt{\sl{int} \tt{last\sub{done}}}
%
%  \textsf{Stores the index of the block in the root such that the process that owns this leaf has most recently finished the. A block is finished if all of its operations are finished. \tt{enqueue(e)} is finished if \tt{e} is returned by some \tt{dequeue()} and \tt{dequeue()} is finished when it computes its response. \it{put the definitions before the pseudocode}}
%  
%\end{itemize}

\Statex $\blacktriangleright$ \tt{\sl{Block}} 
%\cmt{If \tt{b} is \tt{blocks[i](i!=0)} then \tt{b[-1] is blocks[i-1].}}

\begin{itemize}
%  \item \tt{\sl{int} num\sub{enq}, num\sub{deq}}
%  \textsf{: \# enqueue, dequeue operations in the block}
%  \item \tt{\sl{int} num, sum}
%  \textsf{: total \# operations in block, prefix sum of \tt{num}}

  \item \tt{\sl{int} super}
  \textsf{: approximate index of the superblock, read from \tt{parent.head} when appending the block to the node}
\end{itemize}




\Statex $\blacktriangleright$ \tt{\sl{LeafBlock} extends \sl{Block}}
\begin{itemize}
  \item \tt{\sl{Object} element}
  \textsf{: Each block in a leaf represents a single operation. If the operation is \tt{enqueue(x)} then \tt{element=x}, otherwise \tt{element=null}.}
  
    \item \tt{\sl{int} sum\sub{enq}, sum\sub{deq}}
  \textsf{: \# enqueue, dequeue operations in the prefix for the block}
  
\end{itemize}

\pagebreak

\Statex $\blacktriangleright$ \tt{\sl{InternalBlock} extends \sl{Block}}
\begin{itemize}
    \item \tt{\sl{int} end\sub{left}, end\sub{right}}
  \textsf{:~~indices of the last subblock of the block in the left and right child}
  \item \tt{\sl{int} sum\sub{enq-left}}
  \textsf{: \# enqueue operations in the prefix for \tt{left.blocks[end\sub{left}]}}
  \item \tt{\sl{int} sum\sub{deq-left}}
  \textsf{: \# dequeue operations in the prefix for \tt{left.blocks[end\sub{left}]}}
  \item \tt{\sl{int} sum\sub{enq-right}}
  \textsf{: \# enqueue operations in the prefix for \tt{right.blocks[end\sub{right}]}}
  \item \tt{\sl{int} sum\sub{deq-right}}
  \textsf{: \# dequeue operations in the prefix for \tt{right.blocks[end\sub{right}]}}
\end{itemize}


\Statex $\blacktriangleright$ \tt{\sl{RootBlock} extends \sl{InternalBlock}}
\begin{itemize}
  \item \tt{\sl{int} \size}
  \textsf{: size of the queue after performing all operations in the prefix for this block}
%  \item \tt{\sl{int} sum\sub{non-null deq}}
%  \textsf{: count of non-null dequeus up to this block}
%  \item \tt{\sl{counter} num\sub{finished}}
%  \textsf{: number of finished operations in the block}
%    \item \tt{\sl{int} order}
%  \textsf{: the index of the block in the \tt{BlockList} containing the block.}
\end{itemize}



\end{multicols}
\end{algorithmic}
\end{algorithm}

%##########################################

\begin{footnotesize}
  
%\it{Variable naming:}
%\begin{itemize}
%  \item \tt{b\sub{op}}: index of the block containing operation \tt{op}
%  \item \tt{r\sub{op}}: rank of operation \tt{op} i.e. the ordering among the operations of its type according to linearization ordering
%\end{itemize}

\it{Abbreviations:}
\begin{itemize}
 \item \tt{blocks[b].sum\sub{x}=blocks[b].sum\sub{x-left}+blocks[b].sum\sub{x-right}}  \tt{ (for b$\geq$0 and x $\in$ \{enq, deq\}})
 \item \tt{blocks[b].sum=blocks[b].sum\sub{enq}+blocks[b].sum\sub{deq}}  \tt{ (for b$\geq$0})
  \item \tt{blocks[b].num\sub{x}=blocks[b].sum\sub{x}-blocks[b-1].sum\sub{x}} \\ \tt{(for b>0 and x $\in$ \{$\emptyset$, enq, deq, enq-left, enq-right, deq-left, deq-right\})}
\end{itemize}
\end{footnotesize}

\pagebreak

%##########################################


\begin{algorithm}
\caption{\tt{\sl{Queue}}}
\begin{algorithmic}[1]
\setcounter{ALG@line}{200}
\begin{multicols}{2}

\Function{void}{Enqueue}{\sl{Object} e} \cmt{Creates a \tt{block} with element \tt{e} and adds it to the tree.}
\State \tt{block newBlock= \Call{new}{\sl{LeafBlock}}}
\State \tt{newBlock.element= e}
%\State \tt{b.num\sub{enq}=1}
\State \tt{newBlock.sum\sub{enq}= leaf.blocks[leaf.\head].sum\sub{enq}+1}
\State \tt{newBlock.sum\sub{deq}= leaf.blocks[leaf.\head].sum\sub{deq}}
\State \tt{leaf.}\Call{Append}{newBlock}
\EndFunction{Enqueue}

\Statex

\Function{Object}{Dequeue()}{} \cmt{Creates a block with null value element, appends it to the tree, computes its order among operations, and returns its response.}
\State \tt{block newBlock= \Call{new}{\sl{LeafBlock}}} 
\State \tt{newBlock.element= null}
\State \tt{newBlock.sum\sub{enq}= leaf.blocks[leaf.\head].sum\sub{enq}}
\State \tt{newBlock.sum\sub{deq}= leaf.blocks[leaf.\head].sum\sub{deq}+1}
\State \tt{leaf.}\Call{Append}{newBlock}
\State \tt{<b, i>=} \Call{IndexDeq}{leaf.\head, 1}
\State \tt{output=} \Call{FindResponse}{b, i} 
\label{deqRest}
%\If{\tt{i\sub{enq}==-1}}
%\State \tt{output= null}
%\Else
%\State \tt{output= \Call{GetEnq}{b\sub{enq}, i\sub{enq}}}
%\EndIf
\State \Return{\tt{output}}
\EndFunction{Dequeue}

\pagebreak

\Function{<int, int>}{FindResponse}{\sl{int} b, \sl{int} i}\Statex\cmt{Returns the the response to the $D_{root,b,i}$.}
\If{\tt{ root.blocks[b-1].\size}\tt{ + root.blocks[b].num\sub{enq} - i $<$ 0}} \State \Return \tt{null} \label{checkEmpty}\cmt{Check if the queue is empty.}
\Else
\State \tt{e= i - root.blocks[b-1].size + root.blocks[b-1].sum\sub{enq}} \label{computeE}
\Statex\cmt{$E_e(root)$ is the response.}
\State \Return \tt{root.GetENQ(root.\Call{DSearch}{e, b})}\label{findAnswer}
\EndIf
\EndFunction{FindResponse}

\end{multicols}
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{BlockList}
\begin{algorithmic}[1]
\setcounter{ALG@line}{700}

\Statex \cmt{\textsf{: Supports two operations \tt{blocks.tryAppend(Block b), blocks[i]}. Initially  empty, when \tt{blocks.tryAppend(b, n)} returns true \tt{b} is appended to \tt{blocks[n]} and \tt{blocks[i]} returns $i$th block in the \tt{blocks}. If some instance of \tt{blocks.tryAppend(b, n)} returns \tt{false} there is a concurrent instance of \tt{blocks.tryAppend(b$^\prime$, n)} which has returned \tt{true}.\tt{blocks[0]} contains an empty block with all fields equal to 0 and \tt{end\sub{left}, end\sub{right}} pointers to the first block of the corresponding children.}}

%\Statex
%\Statex $\diamondsuit$ \tt{\sl{root implementation}}
%%  \Statex \sf{A persistant red-black tree supporting \tt{append(b, key),get(key=i),split(j)}}.
%%  \tt{append(b, key) returns \tt{true} in case successful. Since \tt{order, sum\sub{enq}}are both strictly increasing we can use one of them for another.}
%%\Statex \tt{root: }\sf{pointer to the root of the PBRT}
%\Function{boolean}{TryAppend}{\sl{block} blk, \sl{int} n} \cmt{\textsf{adds block b to the \tt{root.blocks[n]}}}
%\If{\tt{root.size\%$p^2$==0}} \cmt{Help every often $p^2$ operations appended to the root.}
%\For{\tt{leaf l} \bf{in} \tt{tree leaves}}
%\State \tt{l.Help()}
%\EndFor
%\EndIf
%\State \tt{blk.num\sub{finished}= 0}
%%\State \tt{*oldRoot= \&root.blocks.root}
%%\State \tt{*newRoot= root.blocks.Append(blk).root}
%%\State \Return \tt{CAS(root, oldRoot, newRoot)}
%\State \Return{\tt{CAS(blocks[n], null, blk)}}
%\EndFunction{TryAppend}

\Statex
%\Statex $\diamondsuit$ \tt{\sl{Array implementation}}
\Statex \tt{\sl{block[]} blocks: }\sf{array of blocks}
\Statex
\Function{boolean}{TryAppend}{\sl{block} blk, \sl{int} n} 
\State \Return{\tt{CAS(blocks[n], null, blk)}}
\EndFunction{TryAppend}


\end{algorithmic}
\end{algorithm}
%##########################################


\begin{algorithm}
\caption{\tt{\sl{Node}}}
\begin{algorithmic}[1]
\setcounter{ALG@line}{300}
\begin{multicols}{2}

\Function{void}{Propagate()}{}
\If{\bf{not} \Call{Refresh()}{}} \label{firstRefresh}
\State \Call{Refresh()}{} \label{secondRefresh}
\EndIf
\If{\tt{this} \bf{is not} \tt{root}}
\State \tt{parent.}\Call{Propagate()}{}
\EndIf
\EndFunction{Propagate}

\Statex

\Function{boolean}{Refresh()}{}
\State \tt{h= \head} \label{readHead}
%\State \tt{hp= parent.\head} \label{readParentHead1}
%\If{\nf{blocks[h]!=null}}
%\State \tt{CAS(blocks[h].super, null, hp)} \label{helpSuper1}
%\EndIf
\ForEach{\tt{dir} {\keywordfont{in}} \tt{\{left, right\}}} \label{startHelpChild1}
\State \tt{h\sub{dir}= dir.\head} \label{readChildHead}
\If{\nf{dir.blocks[h\sub{dir}]!=null}}
\State{\tt{dir.\Call{Advance}{h\sub{dir}, h}}} \label{helpAdvance}
\EndIf
\EndFor \label{endHelpChild1}
\State \tt{new= \Call{CreateBlock}{h}} \label{invokeCreateBlock}
\If{\tt{new.num==0}} \Return{\tt{true}} \label{addOP} 
%\cmt{Created block contains nothing.}
\EndIf
\State{\tt{result= blocks[h].CAS(null, new)}} \label{cas}

\State \tt{h\sub{p}= parent.\head} \label{readParentHead}
\State{\tt{this.\Call{Advance}{h, h\sub{p}}}} \label{advance}
%\cmt{Even if another process wins, help to increase the \tt{\head}. The winner might have fallen sleep before increasing \tt{\head}.} \label{incrementHead2}
\State \Return{ \tt{result}}

\EndFunction{Refresh}

\Statex

\Function{void}{Advance}{\sl{int} h, \sl{int} hp}

\State \tt{blocks[h].super.CAS(null, hp)} \label{setSuper1}
\State \tt{head.CAS(h, h+1)} \label{incrementHead}
\EndFunction{Advance}

\pagebreak
\Statex $\leadsto$ \textsf{Precondition: \tt{blocks[start..end]} contains a block with field \tt{f} $\geq$ \tt{i}}
\Function{int}{BSearch}{\sl{field} f, \sl{int} i, \sl{int} start, \sl{int} end}

\Statex \cmt{\textmd{Does binary search for~the value \tt{i} of the given prefix sum \tt{field}. Returns the index of the leftmost block in \tt{blocks[start..end]} whose \sl{field} \tt{f} is $\geq$ \tt{i}}.}
%\State \Return \tt{result block's index}
\EndFunction{BSearch}

\Statex

\Function{<Block, int, int>}{CreateBlock}{\sl{int} i} 
\cmt{Creates and returns the block to be inserted as $i$th \tt{block} in \tt{blocks}.}
\State \tt{block newBlock= \Call{new}{\sl{block}}}
\ForEach{\tt{dir} {\keywordfont{in}} \tt{\{left, right\}}}
\State \tt{index\sub{last}= dir.\head-1} \label{lastLine}
\State \tt{index\sub{prev}= blocks[i-1].end\sub{dir}} \label{prevLine}
\State \tt{newBlock.end\sub{dir}= index\sub{last}} \label{endDefLine}
\State \tt{block\sub{last}= dir.blocks[index\sub{last}]}
\State \tt{block\sub{prev}= dir.blocks[index\sub{prev}]}
\State \cmt{\tt{newBlock} includes \tt{dir.blocks[index\sub{prev}+1..index\sub{last}]}.}
\State \tt{newBlock.sum\sub{enq-dir}= blocks[i-1].sum\sub{enq-dir} + block\sub{last}.sum\sub{enq} - block\sub{prev}.sum\sub{enq}}
\State \tt{newBlock.sum\sub{deq-dir}= blocks[i-1].sum\sub{deq-dir} + block\sub{last}.sum\sub{deq} - block\sub{prev}.sum\sub{deq}}
\EndFor
%\State \tt{b.num\sub{enq}= b.num\sub{enq-left} + b.num\sub{enq-right}}
%\State \tt{b.num\sub{deq}= b.num\sub{deq-left} + b.num\sub{deq-right}}
%\State \tt{b.num= b.num\sub{enq} + b.num\sub{deq}}
%\State \tt{b.sum= n.blocks[i-1].sum + b.num}

\If{\tt{this} \bf{is} \tt{root}}
\State \tt{newBlock.\size= max(root.blocks[i-1].\size { }+ newBlock.num\sub{enq} - newBlock.num\sub{deq}, 0)}\label{computeLength}
%\State \tt{b.sum\sub{non-null deq}= root.blocks[i-1].sum\sub{non-null deq} + max( b.num\sub{deq} - root.blocks[i-1].length - b.num\sub{enq}, 0)}
\EndIf

\State \Return \tt{<b, np\sub{left}, np\sub{right}>}
\EndFunction{CreateBlock}

\end{multicols}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Root}
\begin{algorithmic}[1]
\setcounter{ALG@line}{800}
\Statex
\Statex $\leadsto$ \textsf{Precondition: \tt{root.blocks[end].sum\sub{enq} $\geq$ \tt{e}}}
\Function{<int, int>}{DSearch}{\sl{int} e, \sl{int} end}
\cmt{Returns \tt{<b,i>} if $E_e(root)=E_i(root,b)$.}
\State \tt{start= end-1} \label{dsearchStart}
\While{\tt{root.blocks[start].sum\sub{enq}}$\geq$\tt{e}}
\State \tt{start= max(start-(end-start), 0)} \label{doubling}
\EndWhile
\State \tt{b= root.BSearch(sum\sub{enq}, e, start, end)} \label{dsearchEnd}
\State \tt{i= e- root.blocks[b-1].sum\sub{enq}} \label{DSearchComputei}
\State\Return \tt{<b,i>}
\EndFunction{DSearch}
\end{algorithmic}
\end{algorithm}

%##########################################

\begin{algorithm}
\caption{Node}
\begin{algorithmic}[1]
\setcounter{ALG@line}{400}

\Statex $\leadsto$ \textsf{Precondition:~\tt{blocks[b].num\sub{enq}$\geq$i$\geq 1$}}
\Function{element}{GetEnq}{\sl{int} b, \sl{int} i} \cmt{Returns the element of $E_i(this,b)$.}
\If{\tt{this} \bf{is} \tt{leaf}}
\State\Return \tt{blocks[b].element} \label{getBaseCase}
\ElsIf{\tt{i $\leq$ blocks[b].num\sub{enq-left}}} \label{leftOrRight} \cmt{$E_i(this,b)$ is in the left child of this node.}
\State \tt{subBlock= left.\Call{BSearch}{sum\sub{enq}, i+blocks[b-1].sum\sub{enq-left}, blocks[b-1].end\sub{left}+1, blocks[b].end\sub{left}}} \label{leftChildGet}
\State \Return\tt{left.}\Call{GetEnq}{subBlock, i} 
\Else
\State \tt{i= i-blocks[b].num\sub{enq-left}}
\State\tt{subBlock= right.\Call{BSearch}{sum\sub{enq}, i+right.blocks[b-1].sum\sub{enq-right}, blocks[b-1].end\sub{right}+1, blocks[b].end\sub{right}}} \label{rightChildGet}
\State \Return\tt{right.}\Call{GetEnq}{subBlock, i} 
\EndIf
\EndFunction{GetEnq}

\Statex
\Statex $\leadsto$ \textsf{Precondition: \tt{b}th block of the node has propagated up to the root and \tt{blocks[b].num\sub{enq}$\geq$i}.}
\Function{<int, int>}{IndexDeq}{\sl{int} b, \sl{int} i} \cmt{Returns \tt{<x, y>} if $D_{this,b,i}=D_{root,x,y}$.}
\If{\tt{this} \bf{is} \tt{root}}
\State\Return \tt{<b, i>} \label{indexBaseCase}
\Else
\State \tt{dir= (parent.left==n)? left: right} \cmt{check if this node is a left or a right child}
\State \tt{superBlock= parent.\Call{BSearch}{sum\sub{deq-dir}, i+blocks[b-1].sum\sub{deq}, blocks[b].super-2, blocks[b].super+2}} \label{computeSuper}
\Statex\cmt{superblock's group has at most $p$ difference with the value stored in \tt{super[]}.}

\If{\tt{dir {\keywordfont is} left}}
\State \tt{i+= blocks[b-1].sum\sub{enq}-blocks[superBlock-1].sum\sub{enq-left}} \cmt{consider the enqueues in the previous blocks from the left child} \label{considerPreviousLeft}
\EndIf

\If{\tt{dir {\keywordfont is} right}}
\State \tt{i+= blocks[b-1].sum\sub{enq}-blocks[superBlock-1].sum\sub{enq-right}} \cmt{consider the enqueues in the previous blocks from the right child} \label{considerPreviousRight}
\State \tt{i+= blocks[superBlock].num\sub{deq-left}} \cmt{consider the dequeues from the right child} \label{considerLeftBeforeRight}
\EndIf
\State \Return\tt{this.parent.}\Call{IndexDeq}{superBlock, i}
\EndIf
\EndFunction{IndexDeq}

\end{algorithmic}
\end{algorithm}


%##########################################

%\begin{algorithm}
%\caption{Root}
%\begin{algorithmic}[1]
%\setcounter{ALG@line}{500}
%
%\Function{Block}{FindMostRecentDone}{}
%\For{\tt{leaf l} \bf{in leaves}}
%\State\tt{max= Max(l.maxOld, max)}
%\EndFor
%\State\Return\tt{max} \cmt{This snapshot suffies.}
%\EndFunction{FindMostRecentDone}
%
%\end{algorithmic}
%\end{algorithm}
%##########################################

\begin{algorithm}
\caption{Leaf}
\begin{algorithmic}[1]
\setcounter{ALG@line}{600}

\Function{void}{Append}{\sl{block} blk} \cmt{Append is only called by the owner of the leaf.}
\State \tt{blk.group= \head} \label{appendStart} 
\State \tt{blocks[\head]= blk} 
\State \tt{\head+=1} \label{appendEnd} 
\State \tt{parent.}\Call{Propagate()}{} 
\EndFunction{Append}

%\Statex
%
%\Function{void}{Help}{}\cmt{Helps pending operations}
%
%\State{\tt{last= l.\head-1}}
%\cmt{\tt{l.blocks[last]} can not be \tt{null} because \head increases after appending, see lines \ref{appendStart}-\ref{appendEnd}.}
%\If{\tt{l.blocks[last].element==null}} \cmt{operation is dequeue}
%\State \tt{l.blocks[last].response= l.}\Call{HelpDequeue()}{}
%\EndIf
%
%\EndFunction{Help}

\end{algorithmic}
\end{algorithm}



%##########################################


%\begin{algorithm}
%\caption{Yet to decide how to handle.}
%\begin{algorithmic}[1]
%\setcounter{ALG@line}{800}
%
%%\Function{void}{CollectGarbage}{}\cmt{Collects the root blocks that are done.}
%%\State \tt{s=FindMostRecentDone(Root.Blocks.root)}  \cmt{Lemma: If block b is done after helping then all blocks before b are done as well.}
%%\State \tt{t1,t2= RBT.split(order, s)}
%%\State \tt{RBTRoot.CAS(t2.root)}
%%\EndFunction{CollectGarbage}
%
%\Function{response}{FallBack}{op i} \cmt{\it{how to use as exception handling? by adding try catch in all the methods reading the root?}}
%
%\If{root.blocks.get(num\sub{enq}), i is null} \cmt{this enqueue was already finished}
%
%\State \Return \tt{this.leaf.response(block.order)}
%\EndIf
%
%\EndFunction{FallBack}
%
%\end{algorithmic}
%\end{algorithm}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
                     %
                     %
                     %
                     %
                     %
                     %
                     %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section{Proof of Correctness}

To prove a shared data structure works correctly it is sufficient to show it is linearizable. In our case which we create the linearization ordering in the root, we need to prove the ordering is legal i.e for every execution on our queue if operation $op_1$ terminates before operation $op_2$ then $op_1$ is linearized before operation $op_2$ and if we do the linearization ordering created on a sequential queue we get the same results as our queue. The proof is structured like this. First we define and prove some facts about blocks and node's head properties. Then we introduce  the linearization ordering formally. Then we prove double \nf{Refresh} on a node is enough to propagate children's new operations up to the node. After this we prove some claims about the size of each block. Having propagates working  in a wait-free manner then we prove the correctness of \nf{DSearch()} and \nf{GetENQ()} and \nf{IndexDEQ}. Having these two main methods we prove the way we compute the response of a dequeue is correct and finally we prove the linearizability.

%\framebox[1.1\width]{TEST} Fix the logical order of definitions (cyclic refrences).
%
%%\framebox[1.1\width]{Questions} When I write the lemmas since every claim in my mind is correct maybe I miss some fact that need proof or maybe I refer to some lemmas that are generally correct but not needed for the linearizability proof. Is lemma 7 necessary? Is lemma 13 induced trivially from lemma 8?
%
%\framebox[1.1\width]{TEST} Is it better to show \nf{ops(EST\sub{n, t})} with \nf{EST\sub{n, t}}?
%
%%\framebox[1.1\width]{TEST} How to merge notions of blocks and operations? block b $\sqsubseteq$ block c means b is subblock of c. block b $\in$ set B means b is in B. Merge these two to have shorter formulaes.
%
%\framebox[1.1\width]{Question} A good notation for \it{the index of the \nf{b}}?
%
%\framebox[1.1\width]{Question} How to remove the notion of time? To say pre(n,i) contains n.blocks[0..i] instead of EST(n,t) which head=i at time t. Is it good? Furthermore, can we remove the notion of established blocks?

A block is an object storing some statistics, as described in Algorithm Queue. A block in a node's blocklist implicitly represents a set of operations.
\begin{definition}[Ordering of a block in a node]
Let $b$ be \nf{n.blocks[$i$]} and $b^\prime$ be \nf{n.blocks[$j$]}. We call $i$ the \emph{index} of block $b$. Block $b$ is \it{before} block $b^\prime$ in node \nf{n} if and only if $i<j$. We define \it{the prefix for} $b$, to be the blocks in the \tt{n.blocks[0..$i$]}.
\end{definition}

Next we show that \nf{head} in a node can only be increased and when a \nf{Refresh} terminates \nf{head} is incremented by the process itself or maybe helped by another process.
\begin{observation} \label{nonDecreasingHead}
  \nf{head} is non-decreasing over time.
\end{observation}
\begin{proof}
  The claim follows trivially from the code since \nf{head} is only changed by incrementing in Line \ref{incrementHead} of \nf{Advance()} .
\end{proof}
\begin{lemma}[head Increment] \label{lem::headInc}
Let $R$ be an instance of \nf{Refresh} on node \nf{n}. After $R$ terminates \nf{n.head} is greater than the value read in line \ref{readHead} of $R$.
\end{lemma}
\begin{proof}
If the \nf{CAS} in Line \ref{incrementHead} is successful then the claim holds. Otherwise it has changed from the value that was read in Line \ref{readHead}, which by Observation \ref{nonDecreasingHead} it means another process has incremented the head.
\end{proof}

Now we show \nf{n.blocks[n.head]} is the last  block written into \nf{n} or the first empty block in \nf{n}.

\begin{invariant}[headPosition] \label{lem::headPosition} If the value of \nf{n.head} is $h$ then $\nf{n.blocks[$i$]}=\nf{null}$ for ${i>h}$ and $\nf{n.blocks[i]}\neq\nf{null}$ for ${i<h}$.
\end{invariant}
\begin{proof}
The invariant is true initially since 1 is assigned to \nf{n.head} and \nf{n.blocks[}$x$\nf{]} is null for every $x>0$. The truth of the invariant may be affected by writing into \nf{n.blocks} or incrementing \nf{n.head}. We show the invariant still holds after these two changes.

In the algorithm, \nf{n.blocks} is modified only on Line \ref{cas} which updates \nf{n.blocks[$h$]} for $h$ read from \nf{n.head} in Line \ref{readHead}.Note that since the \nf{CAS} in Line \ref{cas} is successful it means \nf{n.head} has not changed from $h$ till doing the \nf{CAS}. Writing into \nf{n.blocks[n.head]} preserves the invariant, since the claim does not talk about the content of \nf{n.blocks[n.head]}.

The value of \nf{n.head} is modified only in Line \ref{incrementHead}. If \nf{n.head} is incremented to $h+1$ it is sufficient to show \nf{n.blocks[$h$]}$\neq$\nf{null}. When a process does Line \ref{incrementHead} it means Line \ref{cas} was finished which either case it is successful or not it means \nf{n.blocks[$h$]} is not \nf{null} anymore. %At the end of every \nf{n.Refresh()} with a block  with \head greater than 0 returned by \nf{CreateBlock()} \nf{n.head} is incremented (Lines \ref{incrementHead1}, \ref{incrementHead2}). If a process went to sleep before incrementing the \nf{established} (line \ref{incrementHead1}), nothing can be appended to \nf{n.blocks} until another process increments \nf{n.establsihed} (Line \ref{incrementHead2}).
\end{proof}


The next lemma is used to prove the subblocks of two blocks in a node are disjoint. (I can't say its use without mentioning subblocks)
\begin{lemma}[headProgress] \label{lem::headProgress}
 If $\nf{n.blocks[}i\nf{]}\neq\nf{null}$ and $i>0$ then $\nf{n.blocks[$i$].end\sub{left}} \geq \nf{n.blocks[$i-1$].end\sub{left}}$ and $\nf{n.blocks[$i$].end\sub{right}} \geq \nf{n.blocks[$i-1$].end\sub{right}}$.
\end{lemma}
\begin{proof}

Consider the block $b$ written into \nf{n.blocks[$i$]} by \nf{CAS} at Line \ref{cas}. $b$ is created by the \nf{CreateBlock($i$)} called at Line \ref{invokeCreateBlock}. Prior to this call to \nf{CreateBlock($i$)}, $\nf{n.head}=i$ at Line \ref{readHead}, so \nf{n.blocks[$i-1$]} is already a non-null value \nf{b}$^\prime$ by Invariant \ref{lem::headPosition}. Thus, the \nf{CreateBlock($i-1$)} that created  $b^\prime$ terminated before the \nf{CreateBlock($i$)} that creates $b$ is invoked. The value written into \nf{$b$.end\sub{left}} at Line \ref{endDefLine} of \nf{CreateBlock($i$)}  was minus one of the value read at Line \ref{lastLine} of \nf{CreateBlock($i$)}. Similarly, the value in \nf{n.blocks[$i-1$].end\sub{left}} was minus one of the value read from $\nf{n.left.head}-1$ during the call to \nf{CreateBlock($i-1$)}. As by Observation \ref{nonDecreasingHead} \nf{n.left.head} is non-decreasing, $b^\prime\nf{.end\sub{left}} \leq \nf{$b$.end\sub{left}}$. The proof for \nf{end\sub{right}} is similar.
\end{proof}

We define the subblocks of a block recursively.
\begin{definition}[Subblock]\label{def::subblock}
A block is a \emph{direct subblock} of $i$th block in node $n$ if it is in \texttt{n.left.blocks[n.blocks[$i-1$].end\textsubscript{left}+1..n.blocks[$i$].end\textsubscript{left}]} or in \texttt{n.right.blocks[n.blocks[$i-1$].end\textsubscript{right}+1..n.blocks[$i$].end\textsubscript{right}]} . Block $b$ is a \emph{subblock} of block $c$ if $b$ is a direct subblock of $c$ or a subblock of a direct subblock of $c$.  We say block $b$ is \it{propagated to node} node $n$ if $b$ is in $n.blocks$ or is a subblock of a block in $n.blocks$.
\end{definition}

\begin{lemma} \label{lem::subblocksDistinct}
Subblocks of any two blocks in node $n$ does not overlap.  
\end{lemma}
\begin{proof}
We are going to prove by contradiction. Assume subblocks of $n.blocks[i]$ and $n.blocks[j]$ overlap which $i< j$. Without loss of generality assume left child subblocks of $n.blocks[i]$ overlap with the left child subblocks of $n.blocks[j]$. Direct left subblocks of $n.blocks[i]$ and $n.blocks[j]$ cannot overlap. Because of Lemma \ref{lem::headProgress} we have $\nf{n.blocks[$i-1$].end\sub{left}} \leq \nf{n.blocks[$i$].end\sub{left}} \leq \nf{n.blocks[$j-1$].end\sub{left}} \leq \nf{n.blocks[$j$].end\sub{left}}$. Since the tree height is finite we can go lower in the tree to reach a node that direct subblcoks of two different blocks in a node overlap.
\end{proof}

We define the operations of a block using the definition of subblocks.
\begin{definition}[Operations of a block]\label{def::ops}
A block $b$ in a leaf represents an \nf{Enqueue(e)} if $b.element=e\neq\nf{null}$ otherwise, if $b.element=\nf{null}$, $b$ represents a \nf{Dequeue()}. The set of operations of block \nf{b} is the union set of the operations in subblocks of \nf{b}. We denote the set of operations of block \nf{b} by \nf{ops(b)}.  We also say \nf{b} contains \nf{op} if \nf{op}$\in$\nf{ops(b)}.
\end{definition}

\begin{definition}[Superblock]
  Block \nf{b} is \it{superblock} of block \nf{c} if c is a direct subblock of b.
\end{definition}
\begin{corollary}
Every block has at most one superblock.
\end{corollary}
\begin{proof}
A block having more than one superblock is contradictory with Lemma \ref{lem::subblocksDistinct}.
\end{proof}

Operations are distinct \nf{Enqueue(e)} or \nf{Dequeue()}s invoked by processes. Next lemma proves that each operation appears once in the blocks of a node.
\begin{lemma}[No Duplicates]\label{append}
If $op$ is in $n.blocks[i]$ then there is no $j\neq i$ such that $op$ is in $ops(n.blocks[j])$.
\end{lemma}
\begin{proof}
We prove this claim using Corollary \ref{lem::subblocksDistinct}. Assume $op$ is in the subblocks of both $n.blocks[i]$ and $n.blocks[j]$. From Corollary \ref{lem::subblocksDistinct} we know these blocks are different. So the leaf blocks containing $op$ are different, but since we assumed operations are distinctive it means one operation cannot be invoked two times. This leads us to contradictory with the hypothesis.
\end{proof}

\begin{definition}
$n.blocks[i]$ is \emph{established} at time $t$ if $n.head>i$. \emph{EST\sub{n, t}} is the set of established blocks  of node \nf{n} at time $t$.
\end{definition}


Now we want to say block of a node grow over time.
\begin{corollary}[Growing blocks]\label{lem::blocksOrder}
  If  time $t<$ time $t^\prime$, then $ops(n.blocks)$ at time $t$ $\subseteq$ $ops(n.blocks)$ at time $t^\prime$.
\end{corollary}
\begin{proof}
Blocks are only appended (not modified) with \nf{CAS} to $n.blocks[n.head]$ and $n.head$ is non-decreasing, so the set of blocks of a node contains the the set of blocks before the \nf{CAS}.
\end{proof}

\begin{corollary}[establishedOrder]\label{lem::establishedOrder}
  If  time $t<$ time $t^\prime$, then \nf{ops(EST\sub{n, t})}$\subseteq$ \nf{ops(EST\sub{n, t$^\prime$})}.
\end{corollary}

\pagebreak

\begin{figure}[hbt]
  \center\includegraphics[width=5.5in]{pics/tree}
  \caption{Order of operations in b: operations in leaves are ordered with numerical order in the drawing.}
\end{figure}

Now we define the ordering store in each node. In the non-root nodes we only need ordering of operations of a type among themselves. Processes are numbered from 1 to $p$ and leaves of the tree are assigned from left to right. We will show in Lemma \ref{blockSize} that there is at most one operation from each process in a given block.
\begin{definition} [Ordering of operations inside the nodes] \label{ordering}

\begin{itemize}
  \item $E(n,b)$ is the sequence of enqueue operations in $ops(n.blocks[b])$ defined recursively as follows. $E(leaf,b)$ is the single enqueue operation in $ops(leaf.blocks[b])$ or an empty sequence if $leaf.blocks[b].num_{enq}=~0$. If $n$ is an internal node, then
\begin{align*} 
E(n,b) =&  E(n.left,n.blocks[b-1].end_{left}+1)\cdot E(n.left,n.blocks[b-1].end_{left}+2)\cdots E(n.left,n.blocks[b].end_{left})\cdot \\ 
&E(n.right,n.blocks[b-1].end_{right}+1)\cdot E(n.right,n.blocks[b-1].end_{right}+2)\cdots E(n.right,n.blocks[b].end_{right})
\end{align*}
  \item $E_i(n,b)$ is the $i$th enqueue in $E(n,b)$.
\item The order of the enqueue operations in the node $n$ is $E(n)=E(n,1)\cdot E(n,2)\cdot E(n,3)\cdots$
\item $E_i(n)$ is the $i$th enqueue in $E(n)$.
  \item $D(n,b)$ is the sequence of dequeue operations in $ops(n.blocks[b])$ defined recursively as follows. $D(leaf,b)$ is the single dequeue operation in $ops(leaf.blocks[b])$ or an empty sequence if $leaf.blocks[b].num\sub{deq}=~0$. If $n$ is an internal node, then
\begin{align*} 
D(n,b) =&  D(n.left,n.blocks[b-1].end_{left}+1)\cdot D(n.left,n.blocks[b-1].end_{left}+2)\cdots D(n.left,n.blocks[b].end_{left})\cdot \\ 
&D(n.right,n.blocks[b-1].end_{right}+1)\cdot D(n.right,n.blocks[b-1].end_{right}+2)\cdots D(n.right,n.blocks[b].end_{right})
\end{align*}
    \item $D_i(n,b)$ is the $i$th enqueue in $D(n,b)$.
\item The order of the dequeue operations in the node $n$: $D(n)=D(n,1)\cdot D(n,2)\cdot D(n,3)...$
\item $D_i(n)$ is the $i$th dequeue in $D(n)$.
\end{itemize}
\end{definition}

\begin{definition}[Linearization] \label{def::lin}
 $L=E(root,1).D(root,1).E(root,2).D(root,2).E(root,3).D(root,3)...$
\end{definition}
\pagebreak


\begin{definition}
Let $^{op}t$ be the time $op$ is invoked and $t^{op}$ be the time $op$ terminates.
  $_{l}t$ is the immediate time before running Line $l$.   $t_{l}$ is the immediate time after running Line $l$. $^{op}t_{l}$ is the immediate time before running Line $l$ of operation $op$.   $t_{l}^{op}$ is the immediate time after running Line $l$ of operation $op$. $v_l$ is the value of \nf{v} immediately before line $l$ for the process we are talking about.
\end{definition} 

\begin{definition}
  An instance of \nf{Refresh()} is \it{successful} if its \nf{CAS} in Line \ref{cas} returns \nf{true}. If a successful instance of \nf{Refresh()} terminates, we say it is \it{complete.}
\end{definition}

In the next Lemmas we show for every \nf{Refresh()} that has has succesuful \nf{CAS} in Line \ref{cas}, all the established operations in the children before the \nf{Refresh} are going to be established in the parent after the \nf{Refresh}.

\begin{lemma}[trueRefresh] \label{lem::trueRefresh}
If $R$ is a succesful instance \nf{n.Refresh()}, then we have $ops(EST_{\nf{n.left}, ^Rt}) \; \cup \; ops(EST_{\nf{n.right}, ^Rt}) \subseteq 
 ops(n.blocks_{t_{\ref{cas}}^R})$.
\end{lemma}
\begin{proof}
%So it remains to show the operations of \nf{ops(EST\sub{n.left, t\sub{i}})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{i}})} - \nf{ops(EST\sub{n, t\sub{i}})}, which we call \it{new operations}, are all in \nf{ops(EST\sub{n, t\sub{t}})}. 

We show $ops(EST_{\nf{n.left}, ^Rt})= ops(\nf{n.left.blocks[0..$n.left.head_{309}-1$]}) \subseteq ops(n.blocks_{\ref{cas}}) = ops(\nf{n.blocks[0..}n.head_{\ref{readParentHead}}])$. %Let $h$ be the value \nf{n.Refresh()} reads from \nf{n.head} at Line \ref{readHead} and $h_{left}$ be the value read from \nf{n.left.head} at Line \ref{lastLine}. 
As \nf{CAS} in Line \ref{cas} returns \nf{true} the block \nf{new} is written into \nf{n.blocks[$n.head_{\ref{readHead}}$]}. By the Definition \ref{def::subblock} for the Subblock  the block \nf{new} contains \nf{n.left.blocks[n.blocks[$n.head_{\ref{readHead}}-1$].end\sub{left}+1..$n.left.head_{\ref{lastLine}}-1$]} (see Lines \ref{prevLine} and \ref{lastLine}).
% \nf{ops(EST\sub{n.left, t\sub{i}})}$\subseteq$\nf{ops(n.left.blocks [0..h\sub{left,read}])}.
After the successful \nf{CAS} in Line \ref{cas} we know all blocks in \nf{n.left.blocks[0..$n.left.head_{\ref{lastLine}}-1$]} are subblocks of \nf{n.blocks[0..$n.head_{\ref{readHead}}$]}. Because of the Lemma \ref{nonDecreasingHead} we have, $n.left.head_{309}-1<n.left.head_{\ref{lastLine}}-1$ and $n.head_{\ref{readHead}}<n.head_{\ref{readParentHead}}$. From Lemma \ref{lem::establishedOrder} the inequality we proved is stronger than what we wanted to prove (TODO: make the lemma 13 claim more general, if time passes n.blocks grows).  The proof for the right child is the same.




% From the code of the \nf{CreateBlock()} this block includes the established blocks in n's children at $t_i$. Since the head in \nf{CreateBlock()} is read after $t_i$. So the new operations are in the block appended to $n$.

%Suppose the previous true returning \nf{n.Refresh()} took place in time $(t_i^\prime-t_t^\prime)$. By induction on the number of succsuful \nf{Refresh()}'s \nf{n} we know that \nf{ops(EST\sub{n.left, t\sub{i}$^\prime$})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{i}$^\prime$})} $\in$ \nf{ops(EST\sub{n, t\sub{t}$^\prime$})}. Operations that are in \nf{ops(EST\sub{n.left, t\sub{i}})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{i}})} $\in$ \nf{ops(EST\sub{n, t\sub{t}})} but not in \nf{ops(EST\sub{n.left, t\sub{i}$^\prime$})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{i}$^\prime$})} $\in$ \nf{ops(EST\sub{n, t\sub{t}$^\prime$})} are in the block returned by \nf{n.CreateBlock()} by lemma 10. 
%By Lemma~\ref{lem::createBlock} \nf{new} contains \nf{n}'s childrens' established blocks after Line \ref{readHead} which is  appended to \nf{n.blocks[]} by \nf{TryAppend} in Line 313.
\end{proof}
\begin{figure}[hbt]
  \center\includegraphics[width=5in]{pics/trueRefresh}
  \caption{New established operations of the left child are in the new block. (TO UPDATE)}
\end{figure}

\begin{corollary}[Weak True Refresh] \label{lem::prectrueRefresh}
If $R$ is a complete instance \nf{n.Refresh()}, then we have $ops(EST_{\nf{n.left}, ^Rt}) \; \cup \; ops(EST_{\nf{n.right}, ^Rt}) \subseteq 
 ops(EST_{\nf{n}, t^R})$.
\end{corollary}
\begin{proof}
The left side is the same as the Lemma \ref{lem::trueRefresh}, so it is sufficient to show when \nf{R} terminates established blocks in \nf{n} are super set of \nf{n.blocks} after the Line \ref{cas}.    Because of Lemma \ref{lem::headInc}  we are sure that the \nf{n.head} is incremented after line \ref{incrementHead}. So the \nf{new} block appended to \nf{n} is established at $t^R$. 
\end{proof}


%\it{Mention this new block is establish in t\sub{t}. last sentece is not complete}

%\begin{lemma}[falseRefresh] \label{lem:falseRefresh}
%Â  Â  If instance \nf{r} of \nf{n.Refresh()} reads value \nf{s} on line \ref{readHead} and then returns \nf{false}, then there is another instance $r^\prime$ of \nf{n.Refresh()} that has performed a successful \nf{TryAppend(new, s)}. A \nf{TryAppend()} is successful if its \nf{CAS} is successful.
%\end{lemma}
%\begin{proof}
%Â  If there is no other concurrent successful \nf{n.Refresh()} then \nf{n.Refresh()} would succeed in Line 313. So there is another \nf{n.Refresh()}, that has appended its block in \nf{n.blocks[h]} after Line 310 of the first \nf{n.Refresh()}. 
%%Otherwise the other \nf{Refresh(n)} should have read \nf{h$^\prime$>h} instead of \nf{h} for \nf{n.head}(Line 52).
%\end{proof}

\pagebreak

\begin{lemma}[Double Refresh] \label{doubleRefresh}
Â  Consider two consecutive instances $R_1$, $R_2$ of \nf{Refresh()} on internal node \nf{n} by a process $p$. If $R_1$ and $R_2$ both fail and return false, then we have \nf{ops(EST\sub{n.left, \sub{R1}t})} $\cup$ \nf{ops(EST\sub{n.right, \sub{R1}t})} $\subseteq$ \nf{ops(EST\sub{n, t\sub{R2}})}.
\end{lemma}
\begin{proof}
Â  

If $R_2$ reads some value greater than $i+1$ in Line \ref{readHead} it means a successful instance of \nf{Refresh()} performed its Line \ref{readHead} after $t_{\ref{readHead}}^{R_1}$ and finished its Line \ref{incrementHead1} or \ref{incrementHead2} before $t_{\ref{readHead}}^{R_2}$, from Lemma~\ref{lem::prectrueRefresh} by the end of this instance \nf{ops(EST\sub{n.left, t\sub{1}})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{1}})} has been propagated.

 Let $R_1$ read $i$ and $R_2$ read $i+1$ from Line \ref{readHead}. As $R_2$'s \nf{TryAppend()} returns \nf{false}, there is another successful instance $R_2^\prime$ of \nf{n.Refresh()} that has done \nf{TryAppend()} successfully into \nf{n.blocks[i+1]} before $R_2$ tries to append. Since $R_2^\prime$ creates the block after reading the value $i+1$ from \nf{n.head} (Line \ref{readHead}) and $R_1$ reads the value $i$ from \nf{n.head} and the \nf{head}'s value is increasing by Lemma \ref{lem::headProgress} then $t_{R2^\prime \; \ref{readHead}} > t_{R1 \; \ref{readHead}} > _{R_1}t$ (see Figure \ref{fig::doubleRefresh}). By Lemma \ref{lem::prectrueRefresh} after $R_2^\prime$'s \nf{CAS} ($t_{\ref{cas}}^{R_2^\prime}$) we have \nf{ops(EST\sub{n.left, t\sub{1}})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{1}})} $\subseteq$ \nf{ops(n.blocks)}. Also by Lemma \ref{lem::headInc} on $R_2$ the value  of \nf{n.head}  head is more than $i+1$ after $R_2^\prime$ terminates, so the block appended by $R_2^\prime$ to \nf{n} is established by then ($\nf{n.head}\geq i+2 > i+1$). To summarize, $_{R_1}t$ is before $R_2^\prime$'s read of \nf{n.head} ($t_{\ref{readHead}}^{R_2^\prime}$) and $R_2^\prime$'s successful \nf{CAS} is before $R_2$'s termination. So, by Lemma \ref{lem::prectrueRefresh}, \nf{ops(EST\sub{n.left, t\sub{1}})} $\cup$ \nf{ops(EST\sub{n.right, t\sub{1}})} $\subseteq$ \nf{ops(EST\sub{n, t\sub{2}})}.
%It is obvious that the \nf{new} constructed by the second Refresh in Line 36 contains the blocks in \nf{new} by $R_1$ which $R_1^\prime$ did not contain, since \nf{n.head} is only increasing (Maybe Lemma~\ref{lem::oldnewOrder}). If $R_2$ succeeds by Lemma~\ref{lem::trueRefresh} the claim holds. If not, it is deduced that \nf{n.blocks[h]} was not \nf{null} before $R_2$'s \nf{CAS}. Furthermore, \nf{n.blocks[h]} was \nf{null} before reading \nf{h} by $R_1$. So there is a successful \nf{Refresh()} after the read of \nf{h} in $R_1$ and before the \nf{CAS} of $R_2$. This \nf{Refresh()} contains all the new established operations before Line 35 (Maybe Lemma~\ref{lem::oldnewOrder}) and by Lemma~\ref{lem::trueRefresh} our claim holds. See Figure~\ref{}.
\end{proof}

\begin{figure}[hbt]
  \center\includegraphics[width=6in]{pics/compactdouble.png}
  \caption{\label{fig::doubleRefresh}$_{R_1}t<$ $t_{\ref{readHead}}^{R_1}$ $<$ incrementing \nf{n.head} from $i$ to $i+1$ $<$ $t_{\ref{readHead}}^{R_2^\prime}$ $<$ $t_{\ref{cas}}^{R_2^\prime}$ $<$ incrementing \nf{n.head} from $i+1$ to $i+2$ $<t_{R_2}$} 
\end{figure}


%Â  Block \nf{new} is created of new established subblocks of children of \nf{n}(Lemma \ref{lem::createBlock}, Line 46). If \nf{CAS} in Line 48 succeeds then by Lemma~\ref{lem::trueRefresh} new established blocks will be in \nf{n}.
 

\begin{corollary}
  \nf{ops(EST\sub{n.left, \sub{\ref{firstRefresh}}t})} $\cup$ \nf{ops(EST\sub{n.right, \sub{\ref{firstRefresh}}t})} $\subseteq$ \nf{ops(EST\sub{n, t\sub{\ref{secondRefresh}})}}
\end{corollary}
\begin{proof}
  If the first \nf{Refresh()} in line \ref{firstRefresh} returns \nf{true} then by Lemma \ref{lem::trueRefresh} the claim holds. Also if first \nf{Refresh()} failed and the second \nf{Refresh()} succeeded the claim still holds by Lemma \ref{lem::trueRefresh}. Finally, if both failed the claim is satisfied by Lemma \ref{doubleRefresh}.
\end{proof}

\pagebreak


\begin{corollary}[Propagate Step] \label{doublyRefresh}
All operations in \nf{n}'s children's established blocks before running line \ref{firstRefresh}  of a \nf{Propagate} routine are guaranteed to be in \nf{n}'s established blocks after line~\ref{secondRefresh}.
\end{corollary}
\begin{proof}
If \ref{firstRefresh} or \ref{secondRefresh} succeed, the claim is true by Lemma \ref{lem::trueRefresh}. Otherwise Lines \ref{firstRefresh} and \ref{secondRefresh} satisfy the preconditions of Lemma \ref{doubleRefresh}.
\end{proof}

\begin{corollary}\label{lem::appendExactlyOnce}
  After \nf{Append(blk)} finishes \nf{ops(blk)}$\subseteq$\nf{ops(root.blocks[x])} for exactly one \nf{x}.
\end{corollary}
\begin{proof}
After \nf{Append(blk)}'s termination, \nf{blk} is in \nf{root.blocks} since \nf{blk} is established  in the leaf it has been added to. By applying Lemma \ref{doublyRefresh} inductively it is propagated up to the root. Finally Lemma \ref{append} shows  only one block in the root contains \nf{blk}.
\end{proof}


\begin{lemma}[Block Size Upper Bound]\label{blockSize}
Each block contains at most one operation of each processs.
\end{lemma}
\begin{proof}
To derive a contradiction, assume there are two operations $op_1$ and $op_2$ of process $p$ in block $b$ in node $n$. Without loss of generality $op_1$ is invoked earlier than $op_2$. A process cannot invoke more than one operations concurrently, so $op_1$ has to be finished before $op_2$. By Corollary \ref{lem::appendExactlyOnce}, before appending $op_2$ to the tree $op_1$ exists in every node on the path from $p$'s leaf to the root, because $op_1$'s \nf{Append} is finished before $op_2$'s \nf{Append} starts. So, there is some block $b^\prime$  before $b$ in $n$ containing $op_1$.  Existence of $op_1$ in $b$ and $b^\prime$ contradicts Lemma \ref{append}.
\end{proof}

\begin{lemma}[Subblocks Upperbound]\label{subBlocksBound}
Each block has at most $p$ direct subblocks.
\end{lemma}
\begin{proof}
The claim follows directly from Lemma \ref{blockSize} and the observation that each block appended to the tree contains at least one operation, due to the test on Line~\ref{addOP}. We can also see the blocks in the leaves have exactly one operation in the \nf{Enqueue()} and \nf{Dequeue()} routines.
\end{proof}


\pagebreak

\begin{lemma}[Get correctness] \label{get}
If $\nf{n.blocks[b].num\sub{enq}} \geq \nf{i}$ then \nf{n.GetENQ(b,i)} returns the \nf{element} enqueued by $E_i(n,b)$.
\end{lemma}
\begin{proof}
We are going to prove this lemma by induction on the height of node \nf{n}. For the base case, \nf{n} is a leaf. Leaf blocks each contain exactly one operation, so by the hypothesis, only \nf{n.GetENQ(b,1)} can be called and only when \nf{n.blocks[b]} contains an enqueue. At Line \ref{getBaseCase}, \nf{n.GetENQ(b,1)} returns the \nf{element} of the \nf{enqueue} operation stored in the $b$th block of leaf \nf{n} .

For the induction step we prove \nf{n.GetENQ(b, i)} returns $E_i(n,b)$, assuming \nf{n.child.GetENQ(subblock, i)} returns $E_i(n.child,b)$. We argue that  Line \ref{leftOrRight} correctly decides whether the \nf{i}th enqueue in  \nf{b}th block  of internal node \nf{n} is in the  left child or right child subblocks of \nf{n.blocks[b]}. From Definition  \ref{ordering} of $E(n,b)$ we know enqueue operations in a block are ordered from left to right and since the leaves of the tree are ordered by process id from left to right, thus operations from the left subblocks come before operations from the right subblocks in a block (See Figure \ref{figGet}). Furthermore the \nf{num\sub{enq-left}} field  in \nf{n.blocks[b]} stores the number of \nf{enqueue()} operations from the blocks's subblocks in the left child of \nf{n}. So the $i$th enqueue operation is propagated from the right child if \nf{i} is greater than \nf{b.num\sub{enq-left}}. Otherwise we should search for the $i$th enqueue in the left child. By definition \ref{def::subblock} and \ref{def::ops} we need to search in subblocks of \nf{n.blocks[b]} from the range \texttt{n.left.blocks[n.blocks[i-1].end\textsubscript{left}+1..n.blocks[i].end\textsubscript{left}]} $\cup$ \texttt{n.right.blocks[n.blocks[i-1].end\textsubscript{right}+1..n.blocks[i].end\textsubscript{right}]}.

If the $i$th \nf{enqueue} of \nf{n.blocks[b]} is in the left child it would be $i$th enqueue in \texttt{n.left.blocks[n.blocks[i-1].end\textsubscript{left}+1..n.blocks[i].end\textsubscript{left}]} by Definition \ref{def::subblock}. Also, we know there are $eb=n.blocks[b-1].sum_{enq-left}$ enqueues in the blocks before this range, so $E_i(n,b)$ is $E_{i+eb}(n.left)$ which is $E_{i^\prime}(n.left,b^\prime)$ for some $b^\prime$ and $i^\prime$. We can compute $b^\prime$ and then search for $i+eb$th enqueue in \tt{n.left}, where $i^\prime$ is \tt{i+eb-n.left.blocks[}$b^\prime-1$\tt{].sum\sub{enq}}. The parameters in Line \ref{leftChildGet} are for searching $E_{i+eb}(n.left)$ in \nf{n.left.block} in the expected range of blocks, so this \nf{BSearch} returns the index of the subblock containing $E_i(n,b)$.

Otherwise the enqueue we are looking for is in the right child. Then, there are \nf{n.blocks[b].num\sub{enq-left}} enqueues ahead of it in \nf{n.blocks[b]} but not in \texttt{n.right.blocks[n.blocks[i-1].end\textsubscript{right}+1..n.blocks[i].end\textsubscript{right}]}. So we need to search for \nf{i-n.blocks[b].num\sub{enq-left}+ n.blocks[b-1].sum\sub{enq-right}} (Line \ref{rightChildGet}). Other parameters for the left child are chosen similarly to the way they were chosen for the right child. 


So, in both cases the direct subblock containing $E_i(n,b)$ is computed in Lines \ref{leftChildGet} and \ref{rightChildGet}.
 Finally, \nf{n.child.GetENQ(subblock, i)} is invoked on the subblock containing $E_i(n,b)$ and it returns $E_i(n,b)$ by the hypothesis of the induction.
\end{proof}

\begin{figure}[hbt]  
  \center\includegraphics[width=6in]{pics/blockSumEnq.png}
  
  \caption{The number and ordering of the enqueue operations propagated from the left and the right child to \nf{n.blocks[b]}. Enqueue operations from the left subblocks (colored red), are ordered before the enqueue operations from the right child (colored blue).
  }\label{figGet}
\end{figure}

%\textit{I'm not sure it is going to be long and boring to talk about the parameters, since the reader can find out them.}

\pagebreak


\begin{lemma}[DSearch correctness] \label{dsearch}
If $\nf{root.blocks[end].sum\sub{enq}} \geq \nf{e}$, \nf{DSearch(e, end)} returns \nf{<b, i>} such that $E_i(root,b)= E_e(root)$.
\end{lemma}
\begin{proof}
\nf{DSearch} performs a doubling search from \nf{root.blocks[end]} to \nf{root.blocks[0]} to find $E_e(root)$. From Lemma we know \nf{sum\sub{enq}} fields of nf{root.blocks[]} are sorted in a non-decreasing order. Since \nf{root.blocks[0].sum\sub{enq}=0} and there is a \nf{block} in the root with \nf{sum\sub{enq}} value greater than \nf{e}, so there is a \nf{b} that $\nf{root.blocks[b].sum\sub{enq}} \geq e$ but $\nf{root.blocks[b-1].sum\sub{enq}} < e$. This block contains $E_i(root,b)$ and  the search on Line \ref{dsearchStart}-\ref{dsearchEnd} will eventually reach the \nf{b}.

\end{proof}
\begin{lemma}[DSearch Analysis]\label{dsearchTime}
  Assume $\nf{root.blocks[end].sum\sub{enq}} \geq \nf{e}$ and $E_e(root)$'s \nf{element} is the response to some \nf{Dequeue()} operation in \nf{root.blocks[end]}, then  \nf{DSearch(e, end)} takes $\Theta(\log$\nf{root.blocks[b].\size+ root.blocks[end].\size}$)$ steps.
\end{lemma}
\begin{proof}
First we show $\nf{end} - \nf{b} \leq 2 \times \big(\nf{root.blocks[b].\size}+\nf{root.blocks[end].\size}+1 \big)$. From line \ref{addOP}, we know that \nf{num} field of the every block in the tree is greater than 0. So, each block in \nf{root.blocks[b..end]} contains at least one \nf{Enqueue} or at least one \nf{Dequeue}. Suppose there were more than \nf{root.blocks[b].\size}\nf{Dequeue}s in \nf{root.blocks[b+1..end-1]}. Then the element in the queue which is the response to the \nf{Dequeue()} would become dequeued at some point after \nf{blocks[b]}'s last operations and before \nf{root.blocks[end]}'s first operation. Which means the response to to a \nf{Dequeue} in \nf{root.blocks[end]} could not be in $E(n,b)$. Furthermore since the size of the queue would become \nf{root.blocks[end].\size} after the operations of \nf{root.blocks[end]}, there cannot be more than \nf{root.blocks[b].\size + root.blocks[end-1].\size} \nf{Enqueue}s in \nf{root.blocks[b+1..end-1]}., because there can be at most \nf{root.blocks[b].\size}\nf{Dequeue}s and the final size of the queue is \nf{root.blocks[end-1].\size}. Overall there can be at most $2 \times$\nf{root.blocks[b].\size+ root.blocks[end].\size} operations in \nf{root.blocks[b+1..end-1]} and since each block size is $\geq 1$ thus there are at most $2 \times$\nf{root.blocks[b].\size+ root.blocks[end].\size} blocks in between \nf{root.blocks[b]} and \nf{root.blocks[end]}. So $\nf{end-b}\leq 2 \times\nf{root.blocks[b].\size}+\nf{root.blocks[end].\size}+1$. See Figure \ref{fig::doubling}.

Now that we know there are at most \nf{root.blocks[b].\size+root.blocks[end].\size} blocks in between \nf{root.blocks[b]} and \nf{root.blocks[end]} then with doubling search in $\Theta \big(\log($\nf{root.blocks[b].\size+root.blocks[end].\size}$)\big)$ steps we reach \nf{start=c} that the \nf{root.blocks[c].sum\sub{enq}} is less than \nf{e} and \nf{end-c} is not more than $2 \times 2 \times \big(\nf{root.blocks[b].\size+root.blocks[end].\size}\big)$. Beause otherwise, then \nf{(end-c)/2} satisfied the $\nf{root.blocks[(end-c)/2].sum\sub{enq}}<\nf{e}$. In line \ref{doubling} the difference between \nf{end} and \nf{start} is doubled. See Figure \ref{fig::doubling}.

 After computing \nf{b}, the value \nf{i} is computed via the definition of \nf{sum\sub{enq}} in constant time (Line \ref{DSearchComputei}). So the whole \nf{DSearch} routine takes $\Theta\big(\log(\nf{root.blocks[b].\size+root.blocks[end].\size})\big)$ steps.

\end{proof}
\begin{figure}[hbt]  
  \center\includegraphics[width=6in]{pics/doubling.png}
  \caption{Distance relations between $b,c,end$}
  \label{fig::doubling}
\end{figure}

%\begin{figure}[hbt]  
%  \center\includegraphics[width=3in]{pics/end-b.png}
%
%  \caption{The number written on top of the bars is the queue size. the first block is $b$ and the last block is $end$.}
%  \label{end-b}
%\end{figure}

\pagebreak

%\begin{definition}
%An enqueue operation is \textit{finished} if its argument is returned by some process. A dequeue operation is \texttt{finished} if it returns \nf{null} or some value. Block \nf{b} is \textit{done} if all operations in \nf{ops(b)} are finished.
%\end{definition}
%\textit{Problem: we increment the \nf{num\sub{finished}} before returning and after the computing response. How to articulate the sentence above in a not confusing correct way?}
%
%\begin{lemma}[help]\label{help}
%After that \nf{TryAppend()} who is helping finishes, prefix for the blocks of \nf{root.blocks[root.FindMostRecentDone]} are done.
%\end{lemma}
%
%\begin{lemma} Let $n.propagates$ be the number of groups of blocks that have been propagated from node \nf{n} to its parent (successful \nf{n.parent.Refresh()}).
%We have \nf{num\sub{propagated}}$\leq n.propagates\leq$\nf{num\sub{propagated}+p}. \nf{p} is the number of processes.
%\end{lemma}
%\begin{proof}
%    \tt{num\sub{propagated}} is incremented after propagating (Line \ref{incNP}). Since maybe some process falls sleep before incrementing \tt{num\sub{propagated}} it may be behind by \nf{p}.
%\end{proof}
%
%
%\begin{lemma} \nf{super[]} preserves order from child to parent; i.e. if in node \nf{n} block \nf{b} is before \nf{c} then \nf{b.group} $\leq$ \nf{c.group}
%\end{lemma}
%\begin{proof} Line \ref{setGroup}. Since  \nf{num\sub{propagated}} is increasing.\end{proof}
%
%\begin{lemma} Let \nf{b, c} be in node \nf{n}, if \nf{b.group} $\leq$ \nf{c.group} then \nf{super[b.group]} $\leq$ \nf{super[c.group]}\end{lemma}
%\begin{proof} Line \ref{setSuper}.\end{proof}

%\begin{lemma}
%The number of the \nf{block}s with \nf{super=i} in a node is $\leq p$.  
%\end{lemma}
%\begin{proof}For the sake of simplicity we assumed all the blocks are propagated from the left child.
%\begin{figure}[hbt]
%  \center\includegraphics[width=5in]{pics/index1}
%\end{figure}  
%\end{proof}


% \item In a propagate step at most 2 different time values are read \\ If there are more than 2 numbers then the smallest number should have been propagated far before.
% \item There are at most $p^2$ blocks with same time value in a node. \\ At most p processes could die before line 27 and each contains at most p elements.
%\begin{lemma} \nf{super[i+1]-super[i]}$\leq p$\end{lemma}
%\begin{proof}
% In a Refresh with successful CAS in line 46, \nf{super} and \nf{counter} are set for each child in lines 48,49. Assume the current value of the counter in node \nf{n} is \nf{i+1} and still \nf{super[i+1]} is not set. If an instance of successful \nf{Refresh(n)} finishes \nf{super[i+1]} is set a new value and a block is added after \nf{n.parent[sup[i]]}. There could be at most $p$ successful unfinished concurrent instances of \nf{Refresh()} that have not reached line 49. So the distance between \nf{super[i+1]} and \nf{super[i]} is less than $p$.
% \begin{figure}[hbt]
%  \center\includegraphics[width=4in]{pics/index2}
%\end{figure}  
%\end{proof}
%\begin{lemma}[super property]\label{superCounter}
%If \nf{super[i] $\neq$ null} in node \nf{n}, then \nf{super[i]} is the index of the superblock of a block with \nf{time=i} in \nf{n.parent.blocks}.
%\end{lemma}



%\begin{proof} \nf{super[i]} is the index of the superblock of a block containing block b, followed by Lemma \ref{superCounter}. \nf{super(b)} is the real superblock of b. \nf{super(t]} is the index of the superblock of the last block with time \nf{t}. If \nf{b.time} is \nf{t} we have:
%$$super[t]-p\leq super[t-1]\leq super(t-1] \leq super(b) \leq super(t+1)\leq super(t+1]\leq super[t]+p$$
%\end{proof}

%\begin{lemma}
%Search in each level of \nf{IndexDeq()} takes $O(\log p)$ steps.  
%\end{lemma}
%\begin{proof}
%Show preconditions are satisfied and the range is $p$.  
%\end{proof}

\begin{definition}\label{orderRefresh}  A Refresh is \it{successful} if it performs a successful \nf{CAS} on Line \ref{cas}. If \nf{Refresh} instance \nf{R\sub{1}} does its \nf{CAS} at Line \ref{cas} sooner than \nf{Refresh} instance \nf{R\sub{2}} we say \nf{R\sub{1}}  \it{happened} before \nf{R\sub{2}}.
\end{definition}

 Let \nf{i} be the value \nf{R\sub{n}}, a successful instance of \nf{Refresh()} on the node \nf{n}, reads from \nf{n.head}. \nf{R\sub{n}} does a successful \nf{CAS(null, new)} into \nf{n.blocks[h]}, where \nf{b} is the \nf{new}. Without loss of generality for the rest of this section assume \nf{n} is the left child of \nf{n.parent}. From now on we say \nf{p} as an abbreviation for \nf{n.parent}. Let \nf{R\sub{p}} be the first successful \nf{p.Refresh()} that reads some value greater than \nf{i} for \nf{left.head} and is contains \nf{b} in its created block \nf{s} in Line \ref{invokeCreateBlock}. From Lemma \ref{lem::appendExactlyOnce} we know there could be only one \nf{p.Refresh()} propagating \nf{b}. \nf{R\sub{p}} does a successful \nf{CAS(null, new)} into \nf{p.blocks[j]}, where \nf{s} is the \nf{new}.

Although other fields of \nf{b} are set while creating it, because the index of the superblock of \nf{b} is not known until it is propagated, \nf{R\sub{n}} cannot set the \nf{super} field of a \nf{b} while creating it. One approach is to set the \nf{super} field of \nf{b} after it is propagated by \nf{R\sub{b}} but this would not be efficient because there might be $p$ subblocks in \nf{s}. However, once \nf{b} is installed, its superblock is going to be close to \nf{n.parent.head} at the time of installation. One idea is that if we know the aproximate position of the superblock of \nf{b} then we can search for the real superblock when we wished to know the superblock of \nf{b} i.e. \nf{b.super} does not have to be the exact location of  the superblock of \nf{b}, but we want it to be close to \nf{j}. We can set \nf{b.super} to \nf{n.parent.head} while creating \nf{b}, but the problem is that there might be many \nf{p.Refresh}es could happen that contain blocks from the right child of \nf{p} and \nf{j} could be arbitrarily (right word?) greater than \nf{b.super}. We set \nf{b.super} to \nf{p.head} after appending \nf{b} to \nf{n.blocks} (Line \ref{setSuper1}). Maybe \nf{R\sub{n}} goes to sleep at some time after installing \nf{b} and before setting \nf{b.super}. In this case the next \nf{Refresh}es on \nf{n} and \nf{n.parent} help fill in the value of \nf{b.super}.
  
%    By Invariant \ref{lem::headPosition}, \nf{n.parent.blocks[hp]} may be \nf{null} or not \nf{null}, if \nf{hp} is the value read from \nf{n.parent.head} (Line \ref{}). 
%First Lines of the \nf{Refresh} routine(Lines \ref to \ref{}) helps the left child and the right child to fill their \nf{super} field. A process may go to sleep, after Line \ref{cas} of its \nf{Refresh} and before Line \ref{setSuper}. So when a block is going to be appended to \nf{n.parent.blocks} we help the children so the super field in the last block of the children does not differ very much form its real value. After this step we are assured for all the blocks in the children that their index is less than the child's \nf{head}, the \nf{super} field is set. Since we first help to fill the \nf{super} and then increment the \nf{head} (Lines \ref{}). From Lemma \ref{lem::trueRefresh} we knwo that blocks whose index are less than \nf{head} are propagated to the parent in a \nf{Refresh} instance. To summarize Line \ref{endHelpChild1} of \nf{n.Refresh()}, \nf{super} field of all the established blocks in \nf{n.left,n.right} is set and $\neq \nf{null}$.

%
%\begin{corollary}
%  Since Line \ref{incrementHead1} is executed after the Lines \ref{setSuper} and \ref{endHelpChild1}, then when \nf{n.head} is increased from \nf{h} to \nf{h+1}, we know that \nf{n.blocks[h]} is set. It might be two cases that the process which did \nf{TryAppend(new,h)} successfully, has incremented \nf{h} to \nf{h+1} or it might went to sleep and another process  which failed to do \nf{TryAppend(new,h)} comes and helps to set \nf{n.blocks[h].super} and increments \nf{n.head}. Or maybe a process that is trying to add a block to \nf{n.parent} after the \nf{TryAppend(new,h)} helps to set the \nf{super} field of \nf{n.blocks[h]}.
%\end{corollary}


%\begin{lemma}
%  After a successful \nf{TryAppend(new,h)}, \nf{n.blocks[h].super} is set before the next succseful \nf{n.Refresh} or the second next succesful \nf{n.parent.Refresh()}. Superblock of \nf{b} is in \nf{n.parent.blocks[b.super-1..b.super+2]}.
%\end{lemma}
%\begin{proof}
%\nf{n.TryAppend(h+1)}cannot happen unless \nf{n.head} is incremented from \nf{h} to \nf{h+1}. The latter case is explained in the next page.  
%\end{proof}



%
%\begin{lemma}\label{superRange}  
%\end{lemma}
%\begin{proof}
%Let us explain the possible cases of setting \nf{super} of block \nf{b} in node \nf{n}. \nf{b.super} is set by an instance of \nf{n.Refresh} or \nf{n.parent.Refresh}. Since Lines \ref{} setting the \nf{super} is done before incrementing the \nf{head}, so if we know \nf{head} has advanced the super has to be set before.
%
%First case is that the \nf{super} field of \nf{b} is set by the same process that appended \nf{b}. There are two possibilities that \nf{n.parent[hp]} is already \nf{null} or not, when the value \nf{hp} is read from \nf{n.parent.head}. We argue how distant is the superblock of \nf{b} from \nf{n.parent.blocks[hp=b.super]}. If the successful \nf{Refresh} that does \nf{n.parent.TryAppend(new,hp+1)}, creates its \nf{new} block after \nf{n.head.CAS(h,h+1)} (Line \ref{}) then \nf{new} contains the not yet propagated established blocks from \nf{n}, including \nf{b} (blocks in \nf{n} with smaller index than \nf{h+1}). Otherwise, the next \nf{Refresh} on \nf{n.parent} which has read \nf{head=hp+2} contains \nf{b}. Because $\nf{t\sub{n.blocks.TryAppend(b,h)}} < \nf{t\sub{n.head.CAS(h,h+1)}} < \nf{t\sub{hp+2=read n.parent.head}} < \nf{t\sub{n.parent.head.CAS{hp+1,hp+2}}}$. See Lemma \ref{lem::trueRefresh}. In the case that \nf{n.parent.blocks[hp]!=null}, when a block is going to be appended to the the \nf{hp+1} index of the parent it is going to contain the \nf{b}. One may wonder why, becuase it has read the parent's \nf{head} value \nf{hp+2} which is greater than \nf{hp+1} and it has to be incremented after \nf{n.bocks.TryAppend(new,h)}. The other case \nf{n.parent[hp]==null} is the same but one bit closer.
%
%The \nf{super} field of \nf{b} is set by some process doing a \nf{Refresh} on \nf{n} after appending \nf{b} to \nf{n.blocks}. This \nf{Refresh} increments \nf{n.head} from \nf{h} to \nf{h+1}, if it was not already incremented (Lines \ref{}). The next successful \nf{Refresh} on \nf{n.parent} which reads \nf{n.head} with some value greater than \nf{h} propagates \nf{b} to \nf{n.parent}. If \nf{n.parent.blocks[hp]} was not null then \nf{n.parent.blocks[hp+1]} or \nf{n.parent.blocks[hp+2]} contains \nf{b}, otherwise the \nf{super} field in \nf{b} is the exact index of the superblock of \nf{b} or \nf{n.parent.blocks[hp+1]} contains \nf{b}. The second next block appended to the parent is going to absolutely contain \nf{b} if the next block appending to \nf{n.parent} did not contain \nf{b} (Lemma \ref{trueRefresh}). One might wonder why the next block appended to the parent might not contain \nf{b} because it might have read the blocks it was propagating before appending \nf{b} to \nf{n.blocks}.
%
%The \nf{super} field is set by the process doing the \nf{Refresh} on the parent. In this case if the \nf{Refresh} is successful then position of the superblock of \nf{b} is exactly equal to the \nf{super} value. But if it was failure it might be one bit before the real value.
%
%\end{proof}

Block \nf{b} is appended to \nf{n.blocks[h]} on Line \ref{cas}. After appending \nf{b}, \nf{b.super} is set on Line \ref{setSuper1} of a call to \nf{Advance} by the same process or another process's \nf{n.Refresh()} or  maybe an \nf{n.parent.Refresh()}. We want to bound how far \nf{b.super} is from the index of \nf{b}'s superblock, which is created by a successful \nf{n.parent.Refresh()} that propagates \nf{b}.



\begin{observation} \label{setSupBeforeIncHead}
After \nf{n.blocks[i].CAS(null, b)} succeeds, \nf{n.head} cannot increase from \nf{i} to \nf{i+1} unless \nf{b.super} is set.
\end{observation}
\begin{proof}
From the Observation \ref{nonDecreasingHead} we know the only change to \nf{n.head} is on Line \ref{incrementHead} which is incrementing. Before an instance of \nf{Advance()} increments \nf{n.head} on Line \ref{incrementHead}, Line \ref{setSuper1} ensures that \nf{n.blocks[head].super} was set at Line \ref{setSuper1}.
\end{proof}

\begin{corollary}
  If \nf{n.blocks[i].super} is \nf{null}, then $\nf{n.head}<\nf{i}$ and \nf{n.blocks[i+1]} is \nf{null}.
\end{corollary}
\begin{proof}If \nf{b.super} is \nf{null} then \nf{n.head} cannot advance so the next \nf{n.Refresh()} will fail. By the previous corollary, \nf{b.super} has to be set before the next successful Refresh() on n after \nf{R\sub{n}}.  
\end{proof}
Now let us talk about how the \nf{p.Refresh}es that took place after the putting \nf{b} into \nf{n}, will help to set \nf{b.super} and propagate \nf{b}.

\begin{lemma}
  If $\nf{b}\in\nf{n.parent.blocks[i]}$ then $\nf{b.super}\leq\nf{i}$.
\end{lemma}
\begin{proof}
For \nf{R\sub{p}} to contain block \nf{b}, it has to read \nf{n.head} greater than \nf{h} (see Line \ref{lastLine}). For \nf{n.head} to be greater than \nf{h} it means \nf{n.head} is incremented in Line \ref{incrementHead} which means \nf{b.super} was already set in Line \ref{setSuper1} (see Observation \ref{setSupBeforeIncHead}). So if \nf{R\sub{p}} propagates \nf{b} it means \nf{b.super} was already set. Let \nf{j} be the value written in \nf{b.super}. \nf{j} has been read in Line \ref{readHead} or Line \ref{readParentHead} which both are before calling \nf{Advance} that sets \nf{b.super}. From Observation \ref{nonDecreasingHead} we know \nf{p.Head} is non-decreasing so $\nf{j}\leq\nf{i}$. The reader may wonder when the case $\nf{j}=\nf{i}$ happens, it happens when \nf{p.blocks[j]=null} while \nf{j} is read and \nf{R\sub{p}} puts its created block into \nf{n.blocks[j]}.
\end{proof}

\begin{lemma}
If \nf{R\sub=n.Refresh()} puts \nf{b} in \nf{n.blocks[h]} at Line \ref{cas}, then the block created by one of the next two successful \nf{p.Refresh}es according to the Definition \ref{orderRefresh} contains \nf{b} and \nf{b.super} is set before Line \ref{invokeCreateBlock} of the the second successful \nf{p.Refresh()}.
\end{lemma}

\begin{proof}
It is sufficient to prove one of the two successful \nf{p.Refresh()}es propagate \nf{b}. If the first successful \nf{p.Refresh()} propagated \nf{b} then the claim is true, so in the remaining part we assume the first \nf{p.Refresh()} did not propagate \nf{b} and prove the second \nf{p.Refresh()} propagates \nf{b}.

\nf{b.super} is set by some instance of \nf{Refresh()} on \nf{n} or \nf{p} showed by \nf{R'} and \nf{n.head} is incremented by some \nf{Refresh()} called \nf{R"}. We want to know how great $\nf{j}-\nf{b.super}$ can be. \nf{p.head} is \nf{hp} when \nf{R'} reads it. From Lemma \ref{lem::headProgress} \nf{p.head} could only increase from \nf{hp} to \nf{hp+1} if $\nf{p[hp]}\neq\nf{null}$. In other words there should be a successful \nf{p.Refresh()} for \nf{p.head} to increase. We claim there cannot be another successful \nf{p.Refresh()} after \nf{R'} reads \nf{p.head} and before \nf{R\sub{p}} performs Line \ref{lastLine}.

\begin{figure}[hbt]  
  \center\includegraphics[width=7in]{pics/timeLineSuper1.png}
  \caption{Time relations between \nf{R\sub{n},R\sub{p},R',R"}}
  \label{fig::timeLine1}
\end{figure}

Assume the first successful \nf{p.Refresh()} after $t_{\ref{cas}}^{Rn}$ did not set \nf{b.super}. It might happen maybe because the value read for $h_{left}$ in Line \ref{readChildHead} is less than \nf{i} or maybe $\nf{i}=\nf{h\sub{left}}$ and $\nf{left.blocks[h\sub{left}]}=\nf{null}$, which means \nf{n.head} is advanced but \nf{b} is still not installed in \nf{n.blocks[i]} which means \nf{R\sub{n}} has not reached to the Line \ref{cas}.
  
Let the first successful \nf{p.Refresh()} be \nf{Rp1} and the second next successful \nf{p.Refresh()} be \nf{Rp2}. If \nf{Rp1} reads \nf{x} in Line \ref{readHead}, then \nf{Rp2} has to read \nf{x+1} in Line \ref{readHead} (iduced from \ref{lem::headProgress}, \ref{nonDecreasingHead}). See the timeline in Figure \ref{fig::timeLine2} for two consecutive successful \nf{Refresh()} instances \nf{Rp1, Rp2} on \nf{p}.

\begin{figure}[hbt]  
  \center\includegraphics[width=6in]{pics/timeLineSuper2.png}
  \caption{Rp2 reads p.head after $t_{321}^{Rp1}$, which is after $t_{321}^{Rn}$. Rp2 has to help increment n.head and set b.super.}
  \label{fig::timeLine2}
\end{figure}

\begin{figure}[hbt]  
  \center\includegraphics[width=6in]{pics/timeLineSuper3.png}
  \caption{ The second Refresh on \nf{p} contains \nf{b} and reads $\nf{n.head}>\nf{i}$.}
  \label{fig::timeLine3}
\end{figure}

So \nf{b.super} has set by some process before the second next successful \nf{p.Refresh()} on Line \ref{setSuper1}. Since \nf{i} is read in the Line \ref{readHead} then the \nf{CreateBlock()} in Line \ref{invokeCreateBlock} is going to read  some value fo \nf{left.head} greater than \nf{h} and propagates \nf{b} to \nf{p}. So if \nf{b} was not propagated already we are sure the second next successful \nf{p.Refresh()} propagates \nf{b}.
\end{proof}

\begin{corollary}
If \nf{b} has propagated to \nf{f}, then \nf{b.super} has at most 1 difference with the index of the superblock of \nf{b} in \nf{p}.
\end{corollary}













\pagebreak
\begin{lemma}[Computing SuperBlock]\label{superBlock}
For the \nf{superblock} value computed in line \ref{computeSuper} of \nf{n.IndexDEQ(b,i)} we have \nf{n.parent.blocks[superblock]} contains $D_{n,b,i}$.
\end{lemma}
\begin{proof}
First we show the value read for \nf{super[b.group]} in line 418 is not null.
  Values \nf{np\textsubscript{dir}} read in lines \ref{setNP}, \nf{super} are set before incrementing in lines \ref{setSuper},\ref{incNP}. So before incrementing \nf{num\sub{propagated}, super[num\sub{propagated}]} is set so it cannot be null while reading. Then by Lemma \ref{superRange}if we search in the range $p$, we can find the superblock.

\end{proof}


\begin{lemma}[Index correctness]
 If $\nf{n.blocks[b].num\sub{deq}}\geq\nf{i}$ then \nf{n.IndexDEQ(b,i)} returns the rank in $D(root)$ of $D_{n,b,i}$.
\end{lemma}
\begin{proof}
We will prove this by induction on the distance of \nf{n} from the \nf{root}. We can see the base case where \nf{n} is root is trivial (Line \ref{indexBaseCase}).
  In the non-root nodes \nf{n.IndexDEQ(b,i)} computes the superblock of the $i$th Dequeue in the $b$th block of \nf{n} in \nf{n.parent} by Lemma \ref{superBlock} (Line \ref{computeSuper}). After that the order in $D(n.parent, superblock)$ is computed. Note that by Lemma \ref{blockSize} in each block there is at most one operation from each process and operations of one type are ordered based on the order in the subblocks (See Figure \ref{fig::orderFromSubblocks}). Finally \nf{index()} is called on \nf{n.parent} recursively and it returns the correct response from induction hypothesis. If the operation was propagated from the right child the number of dequeues from the left child are added to it (Line \ref{considerRight}), because the left child operations come before the right child operations (Definition \ref{ordering}).
\end{proof}
\textit{Make sure to show preconditions of all invocation of \nf{BSearch} are satisfied.}

\begin{figure}[hbt]  
  \center\includegraphics[width=4in]{pics/orderFromSubblocks.png}
  \caption{Relation of ordering of operations of a block from its subblocks}
  \label{fig::orderFromSubblocks}
\end{figure}


\pagebreak



%\begin{lemma}[Search Ranges]\label{search}
%  Preconditions of all invocation of \nf{BSearch} are satisfied.
%\end{lemma}
%\begin{proof}
%  
%Line 83: \nf{Get(i)} is called if the result of a dequeue is not null. The search is among all blocks in the root.
%
%Line 88: This search tries to find the ith enqueue, knowing that it is in the left child. Search is done over the left subblocks. The start and end of the range are followed by definition. Line 92 is the same.
%
%Line 101: Here, the goal is to find the superblock. We know the distance between answer and the \nf{super[i]} is at most $p$, since at most $p$ processes could die.
%
%\end{proof}
\begin{definition}
   Assume the operations in $L$ are applied on an empty queue. If element of \nf{enqueue e} is the response to  \nf{dequeue d} then we say \nf{R(d)=e}. If \nf{d} 's response id \nf{null} (queue is empty) then \nf{R(d)=null}.
\end{definition}

\begin{definition}
In an execution on a queue, the dequeue operations that return some value are called \it{non-null dequeues}.  
\end{definition}
\begin{observation} \label{responseToADeq}
In a sequential execution on a queue, $k$th non-null dequeue returns the \nf{element} of $k$th enqueue.
\end{observation}

\begin{lemma}\label{sizeCorrectness}
  \nf{root.blocks[b].\size}is the \size of the queue if the operations in the prefix for the $b$th block in the root are applied with the order of $L$.  
\end{lemma}
\begin{proof}
 need to say? :: If the \size of a queue is greater than 0 then a \nf{Dequeue()} would decrease the \size of the queue, otherwise the \size of the queue remains 0. By definition \ref{ordering} enqueue operations come before dequeue operations in a block in $L$.

We prove the claim by induction on \nf{b}. Base case \nf{b=0} is trivial since the queue is initialy empty and \nf{root.blocks[0].size=0}. For \nf{b=i} we are going to use the hypothesis for \nf{b=i-1}. If there are more than \nf{root.blocks[i-1].size+ root.blocks[i].sum\sub{enq}} dequeue operations in \nf{root.blocks[i]} then the queue would become empty after \nf{root.blocks[i]}. Otherwise we can compute the size of the queue after $b$th block using with this equality \nf{root.blocks[b].size= root.blocks[b-1].size+ root.blocks[b].sum\sub{enq}- root.blocks[b].sum\sub{deq}} (Line \ref{computeLength}). See Table \ref{qhistory} for an example of running some blocks of operations on an empty queue.
\end{proof}

\begin{lemma}[Duality of \#non-null dequeues and \nf{block.size}] \label{numberOfNND}
If the operations are applied with the order of $L$, the number of non-null dequeues in the prefix for a block \nf{b} is \nf{b.sum\sub{enq}-b.\size}
\end{lemma}
\begin{proof}
There are \nf{b.sum\sub{enq}} enqueue operations in the prefix for \nf{b}, then the size of the queue after the prefix for \nf{b} is \nf{\#enqs - \#non-null dequeues} in the prefix for \nf{b}, by Observation 35. So \nf{\#non-null dequeues} is \nf{b.sum\sub{enq}-b.\size}. The correctness of the \nf{block.\size}field is shown in Lemma \ref{sizeCorrectness}.
\end{proof}

\begin{lemma}\label{nullReturn}
\nf{R(D\sub{root,b,i})} is \nf{null} iff \nf{root.blocks[b-1].\size+ root.blocks[b].num\sub{enq}- i <0}.
\end{lemma}


\begin{lemma}[Computing Response] \label{computeHead}
\nf{FindResponse(b,i)} returns \nf{R(D\sub{root,b,i}).element}.
\end{lemma}
\begin{proof}
First note that by Definition \ref{ordering} the linearization ordering  of operations will not change as new operations come so instead of talking about the linearization of operations before the $E_i(root, b)$ we talk about what if the whole operation in the linearization are applied on a queue.

$D_{root,b,i}$ is $D_{root,root.blocks[b-1].sum_{deq}+i}$ from the definition \ref{ordering} and \it{$sum_{enq}$}.  $D_{root,b,i}$ returns \nf{null} if \nf{root.blocks[b-1].\size+ root.blocks[b].num\sub{enq}- i <0} by Lemma \ref{nullReturn} (Line \ref{checkEmpty}). Otherwise if it is $d^\prime$th non-null dequeue in $L$ it returns $d^\prime$th enqueue by Observation \ref{responseToADeq} . By Lemma \ref{numberOfNND} there are \nf{root.blocks[b-1].sum\sub{enq} - root.blocks[b-1].\size} non-null dequeue operations before prefix for \nf{root.blocks[b-1]}. Note that the dequeues in \nf{root.blocks[b]} before the $i$th dequeue are non-null dequeues. So the response is $E_{i - root.blocks[b-1].size + root.blocks[b-1].sum\sub{deq}}(root)$ (Line \ref{computeE}). See figure \ref{computeResponseDetail}.

After computing \nf{e} we can find \nf{b,i} such that $E_i(root,b)=E_e(root)$ using \nf{DSearch} and then find its \nf{element} using \nf{GetEnq} (Line \ref{findAnswer}).
\end{proof}

\begin{figure}[hbt]  
  \center\includegraphics[width=2in]{pics/computeResponseDetail.png}\caption{The position of $E_i(root,b)$.}
\label{computeResponseDetail}
\end{figure}

\begin{table}[hbt]
\centering
  \begin{tabular}{c|c|c|c|c}
    \hline &\texttt{DEQ()} & \texttt{ENQ(5)}, \texttt{ENQ(2)}, \texttt{ENQ(1)}, \texttt{DEQ()}& \texttt{ENQ(3)}, \texttt{DEQ()}&  \texttt{ENQ(4)}, \texttt{DEQ()}, \texttt{DEQ()}, \texttt{DEQ()}, \texttt{DEQ()}\\ \hline
    \#enqueues & 0 & 3 & 1 & 1 \\ \hline
        \#dequeues & 1 & 1 & 1 & 4 \\ \hline
            \#non-null dequeues & 0 & 1 & 2 & 5 \\ \hline
                size & 0 & 2 & 2 & 0 \\ \hline
  \end{tabular}
  \caption{An example of root blocks fields. Blocks are from left to right and operations in the blocks are also from the left to right.}\label{qhistory}
\end{table}


%\begin{proof}
%  \nf{head} is incremented in lines 51, 54 after trying to append a block to the index of the last \nf{head} read. If it was successful, we have to do this, but if it was unsuccessful, it means it has appended to the index before, so we have to update the \nf{head}. If a process dies before line 51, another process will increment \nf{head} in line 54.
%\end{proof} 

\pagebreak

\begin{theorem}[Main]
The queue implementation is linearizable.
\end{theorem}
\begin{proof}
  We choose $L$ in Definition \ref{ordering} to be linearization ordering of operations and prove if we linearize operations as $L$ the queue works consistently.
\end{proof}
%\begin{lemma}
%Operations in a block have a time point in common (There is a time $t$ all the operations are running).
%\end{lemma}

\begin{lemma}[satisfiability]
$L$ can be a linearization ordering.
\end{lemma}
\begin{proof}
To show this we need to say if in an execution, \nf{op\sub{1}} terminates before \nf{op\sub{2}} starts then \nf{op\sub{1}} is linearized before \nf{op\sub{2}}. If \nf{op\sub{1}} terminates before \nf{op\sub{2}} starts it means \nf{op\sub{1}.Append()} is terminated before \nf{op\sub{2}.Append()} starts. From Lemma \ref{append} \nf{op\sub{1}} is in \nf{root.blocks} before \nf{op\sub{2}} propagates so \nf{op\sub{1}} is linearized before \nf{op\sub{2}} by Definition \ref{ordering}.

Once some operations are aggregated in one block they will be propagated together up to the root and we can linearize them in any order among themselves. Furthermore in L we arbitrary choose the order to be by process id, since it makes computations in the blocks faster .
\end{proof}

\begin{lemma}[correctness]
  If operations are applied as $L$ on a sequential queue, the sequence of the responses would be the same as our algorithm.
\end{lemma}

\begin{proof}
\it{Old parts to review}
  We show that the ordering $L$ stored in the root, satisfies the properties of a linearizable ordering.
  \begin{enumerate}
    \item If $op_1$ ends before $op_2$ begins in $E$, then $op_1$ comes before $op_2$ in $T$.\\$\blacktriangleright$ This is followed by Lemma \ref{append}. The time $op_1$ ends it is in root, before $op_2$, by Definition \ref{ordering} $op_1$ is before $op_2$.
    \item Responses to operations in $E$ are same as they would be if done sequentially in order of $L$. \\$\blacktriangleright$ Enqueue operations do not have any response so it does no matter how they are ordered. It remains to prove  Dequeue $d$ returns the correct response according to the linearization order. By Lemma \ref{computeHead} it is deduced that the head of the queue at time of the linearization of $d$ is computed properly. If the Queue is not empty by Lemma \ref{get} we know that the returning response is the computed index element.
  \end{enumerate} 
\end{proof}

\pagebreak


\begin{lemma}[Amortized time analysis]
%  \nf{n.GetEnq(b,i), n.Index(b,i)} take $O(\log^2 p)$ steps. Search in the root may take $O(\log Q+ p^2)$ steps. Helping is done every $p^2$ block appended to the root and takes $p\times \log^2p$ steps. Amortized time consumed for helping by each process is $O(\log^2 p)$.
\nf{Enqueue()} and \nf{Dequeue()}, each take $O(\log^2 p + \log q)$ steps in amortized analysis. Where $p$ is the number of processes and $q$ is the size of the queue at the time of invocation of operation.
\end{lemma}
\begin{proof}
\nf{Enqueue(x)} consists of creating a \nf{block(x)} and appending it to the tree. The first part takes constant time. To propagate \nf{x} to the root the algorithm tries two \nf{Refresh}es in each node of the path from the leaf to the root (Lines \ref{firstRefresh}, \ref{secondRefresh}). We can see from the code  that each \nf{Refresh} takes constant number of steps since creating a block is done in constant time and does $O(1)$ \nf{CAS}es. Since the height of the tree is $\Theta(\log p)$, \nf{Enqueue(x)} takes $O(\log p)$ steps.

A \nf{Dequeue()} creates a block with null value element, appends it to the tree, computes its order among enqueue operations, and returns the response. The first two part is similar to an \nf{Enqueue} operation. To compute the order of a \nf{dqueue} in $D(n)$ there are some constant steps and \nf{IndexDeq()} is called. \nf{IndexDeq} does a search with range $p$ in each level (Lemma \ref{superRange}) which takes $O(log^2 p)$ in the tree. In the \nf{FindResponse()} routine \nf{DSearch()} in the root takes $\Theta(\log$(\nf{root.blocks[b].\size+root.blocks[end].\size}) by Lemma \ref{dsearch}, which is $O(\log$ size of the queue when \nf{enqueue} is invoked$)+\log$ size of the queue when \nf{dequeue} is invoked$)$. Each search in \nf{GetEnq()} takes $O(\log p)$ since there are $\leq p$ subblocks in a block (Lemma \ref{subBlocksBound}), so \nf{GetEnq()} takes $O(\log^2 p)$ steps.

If we split \nf{DSearch} time cost between the corresponding \nf{Enqueue}, \nf{Dequeue}, in amortized we have \nf{Enqueue} takes $O(\log p +q)$ and \nf{Dequeue} takes $O(\log^2 p +q)$ steps.
\end{proof}

\begin{lemma}[CASes invoked]
An \nf{Enqueue()} or \nf{Dequeue()} operation, does at most $4\log p$ \nf{CAS} operations.
\end{lemma}
\begin{proof}
  In each height of the tree at most 2 times \nf{Refresh()} is invoked and  every \nf{Refresh()} has 2 \nf{CAS}es, one in Line \ref{cas} and one in Lines \ref{incrementHead1} or \ref{incrementHead2}.
\end{proof}


\pagebreak
\subsection{Garbage Collection or Getting rid of the infinite Arrays}

\pagebreak
\section{Using Queues to Implement Vectors}
Supporting Append, Read, Write in PolyLog time by modifying Get(Enq) Method. Create a Universal Construction Using our vector

\pagebreak
\section{Conclusion}
possible directions for work

Maybe Stacks

Characterize what datastructure can be used for this approach, we already know: queue, fetch \& Inc, Vectors


\end{document}

