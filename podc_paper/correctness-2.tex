% !TEX root =  podc-submission.tex

\subsection{Correctness}
Unfortunately, there are enough changes to the algorithm that a new proof of correctness
is required.  Its structure mirrors the proof of the original algorithm, but requires additional
reasoning to ensure GC does not interfere with other routines.
The following observation describes
how the set of blocks in the RBT can be modified.

\begin{lemma}\label{RBTupdates}
Suppose a step of the algorithm changes $\var{v}.\fld{blocks}$ from a non-empty tree $T$ to $T'$.
If the set of \fld{index} values  in $T$ is $I$, then the set of \fld{index} values in $T'$ is 
$(I \cap [m-1,\infty))\cup \{\max(I) + 1\}$ for some $m$.
\end{lemma}
\begin{proof}
The RBT of a node is updated only at line \ref{appendLeafGC} or \ref{casGC}.

If line \ref{appendLeafGC} of an \op{Append} operation modifies $\var{v}.\fld{blocks}$,
then \var{v} is a leaf node, and no other process ever modifies $\var{v}.\fld{blocks}$.
$T'$ was obtained from $T$ by calling \op{AddBlock}($T,B$).
$B$ was created either by the \op{Enqueue} or \op{Dequeue} that called \op{Append}.
Either way, $B.\fld{index} = \max(I)+1$.
The \op{AddBlock} that creates $T'$ may optionally \op{Split} the RBT and then add $B$ to it.
So the claim is satisfied.

If line \ref{casGC} of a \op{Refresh} modifies $\var{v}.\fld{blocks}$, then \var{v} is an internal node.
After reading $T$ from $\var{v}.\fld{blocks}$ at line \ref{refreshReadTGC},
then creates the  block \var{new},
and then calls \op{AddBlock}($T,new$) to create $T'$.
Line \ref{createIndexGC} sets $\var{new}.\var{index} = \max(I)+1$.
The \op{AddBlock} that creates $T'$ may optionally \op{Split} the RBT and then add $new$ to it.
So the claim is satisfied.
\end{proof}

Since each RBT starts with a single block with \fld{index} 0, the following is an easy consequence of \Cref{RBTupdates}.
\begin{corollary}\label{nonEmptyIncreasing}
The RBT stored in each node \var{v} is never empty and always stores a set of blocks with consecutive indices.
Moreover, its maximum \fld{index} can only increase over time.
\end{corollary}

Since RBTs are always non-empty, calls to \op{MaxBlock} have well-defined answers.  
Throughout the proof, we use $\var{v}.\blocks[b]$ to refer to the block with \fld{index} $b$ that appeared
in \var{v}'s tree at some time during the execution.
It follows from \Cref{RBTupdates} and \Cref{nonEmptyIncreasing} that 
each time a new block appears in \var{v}'s RBT, its \fld{index} is greater than any block
that has appeared in \var{v}'s RBT earlier.
Thus, $\var{v}.\blocks[b]$ is unique, if it exists.
We also use this notation in the code to indicate that a search of the RBT should be performed for the block
with \fld{index} $b$.
We describe later what should happen if that block is not found by the search because it has been removed
by a GC phase.
\here{remember to come back to this point}

We now establish that definition (\ref{defsubblock}) of a block's subblocks
still makes sense.

%\here{If next obs is only used once, can fold it into the proof that needs it}
%\begin{observation}
%\label{edirGC} 
%If $B$ is a block in the RBT of an internal node \var{v}, then for $\var{dir} \in \{\fld{left},\fld{right}\}$,
%$B.\edir$ was read from the \fld{index} field of some node in the RBT of $\var{v}.\var{dir}$.
%\end{observation}
%\begin{proof}
%The \eleft\ and \eright\ fields are set by reading the \fld{index} field of a node in the appropriate child
%on line \ref{createEndLeftGC} and \ref{createEndRightGC}.
%\end{proof}

We next prove the analogue of Lemma \ref{lem::headProgress}.
\begin{customlemma}{\ref{lem::headProgress}$'$}\label{lem::headProgressGC}
 If $h>0$ and a block with index $h$ has been inserted into $\var{v}.\fld{blocks}$ then 
 $\var{v}.\fld{blocks}[h-1].\fld{end\sub{left}} \leq \var{v}.\fld{blocks}[h].\fld{end\sub{left}}$ and 
 $\var{v}.\fld{blocks}[h-1].\fld{end\sub{right}} \leq \var{v}.\fld{blocks}[h].\fld{end\sub{right}}$.
\end{customlemma}
\begin{proof}
The block $B$ with index $h$ was installed into \var{v}'s RBT by the CAS at line \ref{casGC} 
that changed the tree from $T$ to $T'$.
Before this CAS, line \ref{refreshReadTGC} read the tree $T$ from $\var{v}.blocks$,
line \ref{refreshReadMax} found a block $B'$  with \fld{index} $h-1$ in $T$,
and then line \ref{invokeCreateBlockGC} created the block $B$ with $\fld{index}=h$.
Since $B'$ was already in $T$ before $B$ was created, the
\op{CreateBlock}$(v,b-1)$ that created $B'$ terminated before the 
\op{CreateBlock}$(v,b)$ that created $B$ started.
By \Cref{nonEmptyIncreasing}, the value that line \ref{createEndLeftGC} of \op{CreateBlock}$(v,b-1)$ 
stores in $B'.\eleft$ is less than or equal to the value that line \ref{createEndLeftGC} 
of \op{CreateBlock}$(v,b)$ stores in $B.\eleft$.  
Similarly, the values stored in $B'.\eright$ and $B.\eright$ at line \ref{createEndRightGC} satisfy the claim.
\end{proof}

\Cref{lem::headProgressGC} implies that 
the nodes of an in-order traversal of any RBT have non-decreasing values of
$\eleft$ (and of $\eright$).
Thus, the searches for a block based on \eleft\ or \eright\ values 
at lines \ref{searchsuper1}, \ref{searchsuper2} and \ref{searchsuper3},
which are used to look for the superblock of a node or its predecessor, can be done in logarithmic time.

\Cref{lem::subblocksDistinct}, \Cref{lem::noDuplicates}, \Cref{lem::sum}, \Cref{blockNotEmpty} 
hold for the modified algorithm.  Their proofs are identical to those given in 
Section \ref{sec::basicProperties} since they  depend only on \Cref{lem::headProgress} (which can be replaced by \Cref{lem::headProgressGC})
and the definition of subblocks given in (\ref{defsubblock}).
In particular, \Cref{lem::sum} says that nodes in an in-order traversal of a RBT have non-decreasing
values of \fld{sum\sub{enq}}  so the searches of a RBT based on
\fld{sum\sub{enq}} values in lines
\ref{getChildGC} and \ref{FRsearchGC} can be done in logarithmic time.

Next, we prove an analogue of \Cref{successfulRefresh}.
We say a node \var{v} \emph{contains} an operation
if some block containing the operation has previously appeared in \var{v}'s RBT (even if 
the block has been removed from the RBT by a subsequent \op{Split}).

\begin{customlemma}{\ref{successfulRefresh}$'$}
\label{successfulRefreshGC}
Let $R$ be a call to \op{Refresh}(\var{v}) that performs a successful \op{CAS} on line \ref{casGC} (or terminates at line \ref{addOPGC}).
In the configuration after that CAS (or termination, respectively), \var{v} contains all operations that \var{v}'s children contained 
when $R$ executed line~\ref{refreshReadTGC}.
\end{customlemma}
\begin{proof}
Suppose \var{v}'s child (without loss of generality, $\var{v}.\fld{left}$) contained an operation $op$ 
when $R$ executed line \ref{refreshReadTGC}.
Let $i$ be the index of the block containing $op$ that was in \var{v}'s RBT before 
$R$ executed line \ref{refreshReadTGC}.
We consider two cases.

Suppose $R$'s call to \op{CreateBlock} returns a new block $B'$.  Then, $R$'s \op{CAS} at line \ref{cas} 
installs $B'$ in $\var{v}.\fld{blocks}$.
The \op{CreateBlock} set $B'.\eleft$ to the maximum \fld{index} in $\var{v}.\fld{left}$'s RBT at line \ref{createEndLeftGC}.
By \Cref{nonEmptyIncreasing}, this maximum \fld{index} is bigger than $i$.
By the definition of subblocks, some block in \var{v} contains $B$ as a subblock
and therefore \var{v} contains $op$.

Now suppose $R$'s call to \op{CreateBlock} returns \nl, causing $R$ to terminate at line \ref{addOPGC}.
Let $h$ be the maximum \fld{index} in $T$ plus 1.
By reasoning identical to the last paragraph of \Cref{successfulRefresh}'s proof,
it follows from the fact that $\var{num\sub{enq}}+\var{num\sub{deq}}=0$ at line \ref{testEmptyGC}
that the blocks $\var{v}.\fld{left.blocks}[\var{v}.\fld{blocks}[h-1].\eleft+1..new.\eleft]$ and
$\var{v}.\fld{right.blocks}[\var{v}.\fld{blocks}[h-1].\eright+1..new.\eright]$
contain no operations.
By \Cref{blockNotEmpty}, each block contains at least one operation, so
these ranges must be empty, and $\var{v}.\fld{blocks}[h-1].\eleft \geq \var{new}.\eleft\geq i$.
This implies that the block $B$ containing $op$ is a subblock of some block that has appeared
in \var{v}'s RBT, so $op$ is contained in \var{v}.
\end{proof}

This allows us to show that a double \op{Refresh} propagates operations up the tree, 
as in \Cref{lem::doubleRefresh}.

\begin{customlemma}{\ref{lem::doubleRefresh}$'$}\label{lem::doubleRefreshGC}
Consider two consecutive terminating calls $R_1$, $R_2$ to \op{Refresh}(\var{v}) by the same process.
All operations contained \var{v}'s children when $R_1$ begins
are contained in \var{v} when $R_2$ terminates.
\end{customlemma}
\begin{proof}
If either $R_1$ or $R_2$ performs a successful \op{CAS} at line \ref{casGC} or terminates at line \ref{addOPGC}, the claim follows
from \Cref{successfulRefreshGC}.
So suppose both $R_1$ and $R_2$ perform a failed \op{CAS} at line \ref{casGC}.
Then some other \op{CAS} on $\var{v}.\fld{blocks}$ succeeds between the time each \op{Refresh}
reads $\var{v}.\fld{blocks}$ at line \ref{refreshReadTGC} and performs its \op{CAS} at line \ref{casGC}.
Consider the \op{Refresh} $R_3$ that does this successful \op{CAS} during $R_2$.
$R_3$ must have read $\var{v}.\fld{blocks}$ after the successful \op{CAS} during $R_1$.
The claim follows from \Cref{successfulRefreshGC} applied to $R_3$.
\end{proof}

\Cref{lem::appendExactlyOnce} can then be proved in the same way as in Section \ref{sec::propagating}.
We modified the way \var{super} fields are computed, so we prove an analogue of \Cref{superRelation} to show that \fld{super} fields are (nearly) accurate.

\begin{customlemma}{$\ref{superRelation}'$}\label{superRelationGC}
Let $B=\var{v}.\fld{blocks}[b]$.
  If $\var{v}.\fld{parent.blocks}[s]$ is the superblock of $B$ then $s-1\leq B.\fld{super}\leq s$.
\end{customlemma}
\begin{proof}

\end{proof}



The correctness of \op{GetEnqueue} and \op{indexDequeue}, which are identical to the original version of the implementation are dependent only on the fact that we do not discard blocks needed by those routines.
The following lemmas show this.

\here{prove some lemmas about not throwing away useful information, then restate correctness lemmas about getenq and indexdeq that say they work correctly OR the operation is already propagated/complete.}


\here{next, should probably show that blocks are removed only after they are no longer needed; i.e., every dequeue has its answer (either returned, or written in its response field) and every enqueue has a matching dequeue that has either terminated or has a response written.}
one invariant will be that a null dequeue or a dequeued enqueue appears in the first block (or some later block) of each RBT.
should imply that no operation will need any block before the first block in the RBT
-------

\subsection{Analysis}
\label{sec::GCanalysis}

\here{need to prove a lemma analogous to \Cref{blockSize}}

We first bound the size of RBTs.  Let $q_{max}$ be the maximum size of the queue at any time during the sequential execution given by the linearization $L$.
Recall that GC is done on a node every $G$ times its RBT is updated, and we chose $G$ to be $p^2\ceil{\log p}$.
Part of the proof of the following lemma is similar to the proof of \Cref{dSearchTime}.
%We say the index $b$ \emph{is marked in node \var{v}} if some process has tested
%whether $b>\var{v}.\fld{last}[*]$ at line \ref{marktest1} or \ref{marktest2}.
%Intuitively, a block's index value is marked 
%when a dequeue passes through this block to find the enqueue
%whose argument it should return, or when a dequeue that returns null passes through the block.
%Thus, $b$ gets marked in \var{v} when $\var{v}.\fld{blocks}[b]$ 
%contains either an enqueue whose value is being dequeued or a \nl\ dequeue.
%If the index of block $B$ is marked in \var{v}, we also say $B$ is marked in \var{v}.

\begin{lemma}\label{boundAfterGC}
If the maximum \fld{index} in a node's RBT is a multiple of $G$, then
it contains at most $2q_{max}+4p+1$ blocks.
\end{lemma}
\begin{proof}
Consider the invocation $A$ of \op{AddBlock} that updates a node \var{v}'s RBT with the insertion of a block whose 
\fld{index} is a multiple of $G$.
$A$ performs a GC phase.
Let $C$ be the configuration before $A$ invokes \op{SplitBlock} on line \ref{callSplitBlock}.
That call to \op{SplitBlock} recurses up to the root, where it computes $m$ by reading the \var{last}
array.
Let $L_1$ be the prefix of the linearization $L$ corresponding to blocks $1..m$ of the root.
Let $L_2$ be the next segment of the linearization corresponding to blocks $m+1..\ell$ of the root, where
$\ell$ is the last block added to the root's RBT before $C$.

We first bound the number of operations in $L_2$.

The number of enqueues in $L_2$ whose values are still in the queue at the end of $L_2$ is at most $q_{max}$.
Any enqueue in $L_2$ whose value is not still in the queue at the end of $L_2$
must still be in progress at $C$; otherwise the process that dequeued it would have
set its \var{last} entry to a value greater than the index of the block of the root that contains the enqueue prior to $C$,
contradicting the definition of $m$.
So, there are at most $p$ enqueues in $L_2$ whose values are still in the queue at the end of $L_2$.
Thus, there are at most $q_{max}+p$ enqueues in $L_2$.

If a dequeue in $L_2$ returns a non-null value in the sequential execution $L$,
then the value it returns was either in the queue at the end of $L_1$ or it was enqueued during $L_2$.
Thus, there are at most $q_{max}+(q_{max}+p)$ dequeues in $L_2$ that return non-null values.
Any dequeue in $L_2$ that returns a null value in the sequential execution $L$ must still be in progress
at $C$; otherwise the process that performed the dequeue would have set its \var{last} entry
to a value greater than the index of the block of the root that contains the dequeue prior to $C$, 
contradicting the definition of $m$.
So, there are at most $p$ null dequeues in $L_2$.
Thus, there are at most $2q_{max} + 2p$ dequeues in $L_2$.

$A$'s call to \op{SplitBlock} determines the index $s$ used to split \var{v}'s RBT by following 
\eleft\ and \eright\ pointers from the root down to \var{v}.
So, the block returned is a subblock of block $m$ of the root, unless
at some point along the path of subblocks the subblock has already been removed by a split,
in which case \op{SplitBlock} returns a subblock of a block $m'>m$ in the root.

Next, we bound the number of operations in \var{v}'s blocks that are retained when $A$ does sets
$T'\assign\op{Split}(T,s)$ operation.
Since $T$ was read before $C$, any operation in $T$ is either in progress at $C$ or has been propagated to the root before $C$.\here{cite a lemma about propagation before operation terminates}
Thus, there are at most $p$ operations in $T$ that do not appear in $L_1\cdot L_2$.
All the rest of the operations in blocks of $T'$ have been propagated to blocks $m..\ell$ of the root.
There are at most $p$ operations in block $m$ of the root by \Cref{blockSizeGC}
and we showed above that there are at most $2q_{max} + 2p$ in blocks 
$m+1..\ell$ of the root.
Thus, there are at most $2q_{max}+4p$ operations in blocks of $T'$.
Since each block is non-empty by \Cref{blockNotEmpty}, $T'$ contains at most $2q_{max}+4p$ blocks, and one more 
block is inserted before $A$ sets \var{v}'s \fld{blocks} to the resulting RBT.
\end{proof}


\begin{corollary}\label{RBTbound}
At all times, the size of a node's RBT is $O(q_{max}+p+G)$. 
\end{corollary}
\begin{proof}
Each update to a node's RBT adds at most one block to it, increasing its maximum \fld{index} by 1.
Thus, there are at most $G$ updates since the last time its maximum \fld{index} was a multiple of $G$.
The claim follows from \Cref{boundAfterGC}.
\end{proof}

The following theorem bounds the space that is reachable (and therefore cannot be freed by the environment's garbage collector) at any time.
\here{For later version:  There may be room to improve this bound by a more careful analysis since the bounds so far
didn't use the fact that operations only appear along one path in the tree; in other words
doing a bound on total size of all RBTs at one level of the ordering tree at once, rather than
on each node separately, might reduce it to qlogp + poly(p).  Might also be possible to be
more precise about how it depends on "current" size of queue rather than max size.}

\begin{theorem}\label{spaceBound}
The queue data structure uses a maximum of $O(pq_{max}+p^3\log p)$ words of memory at any time.
\end{theorem}
\begin{proof}
There are $2p-1$ nodes in the ordering tree.  Aside from the RBT, each node uses $O(1)$ memory words.
Each process may hold pointers to $O(1)$ RBTs that are no longer current in local variables.
So the space bound follows from \Cref{RBTbound} and the fact that $G$ is chosen
to be $p^2\ceil{\log p}$.
\end{proof}

Although individual operations may now become more expensive because they have to perform GC,
we show that operations still have polylogarithmic amortized step complexity.

\begin{proposition}
The amortized step complexity of each operation is $O(\log p \log(p+q_{max}))$.
\end{proposition}
\begin{proof}
\here{this proof may need some modification since code was changed to change how split point is computed}
It follows from \Cref{RBTbound} and the fact that $G=p^2\ceil{\log p}$
that all the RBT routines we use to perform \op{Split}, \op{Insert} and searches for
blocks with a particular index or for a \fld{sum\sub{enq}} value (in line \ref{FRsearchGC} or \ref{getChildGC}) can all be done in $O(\log(p+q_{max}))$ steps.

First, we bound the number of steps taken \emph{excluding} the GC phase in line \ref{GCstart}--\ref{GCend}.
An \op{Enqueue} or null \op{Dequeue} does $O(1)$ RBT operations and other work at each level of the tree during \op{Propagate},
for a total of $O(\log p \log(p+q_{max}))$ steps.
A non-null \op{Dequeue} must also search for a block in the root at line \ref{FRsearchGC}
and call \op{GetEnqueue}.  At each level of the tree, \op{GetEnqueue} does $O(1)$ RBT operations (including a search at line \ref{getChildGC}) and $O(1)$ other steps.
Thus, a \op{Dequeue} also takes $O(\log p \log(p+q_{max}))$ steps.

Now we consider the additional steps a process takes while doing GC in line \ref{GCstart}--\ref{GCend}.
If a process does GC in a call to \op{AddBlock}($\var{v},T,B$) where $B$ has \fld{index} $r\cdot G$ for some integer $r$, we call this the process's \emph{$r$th helping phase on \var{v}}.
We argue that each process $P$ can do an $r$th helping phase on \var{v} at most once as follows.
Consider $P$'s first call $A$ to \op{AddBlock} that does an $r$th helping phase on \var{v}.
Let $\var{v},T,B$ be the arguments of $A$.
Any call to \op{AddBlock} on internal node \var{v} is from line \ref{refreshABGC} of \op{Refresh}, so $B.\fld{index}$
is the maximum \fld{index} in $T$ plus 1.
The \op{Refresh} that called $A$ performed a \op{CAS} at line \ref{casGC}.  Either the \op{CAS} succeeds
or it fails because 
some other \op{CAS} changes \var{v}.\fld{blocks} from $T$ to another tree.
Either way, by \Cref{RBTupdates}, \var{v}'s RBT's maximum \fld{index} will be at least $rG$ at all times
after this CAS.
So if a subsequent \op{Refresh} by process $P$ 
ever calls \op{AddBlock} on \var{v} again, the block it passes as the third argument
will have $\index>rG$, so $P$ will not perform an $r$th helping phase on \var{v} again.

Each helping phase takes $O(p)$ steps to read the \var{last} array,
$O(p \log p \log(p+q_{max}))$ steps to \op{Help},
followed by $O(\log(p+q_{max}))$ steps to split and insert a new node into a RBT.
Thus, for each integer $r$ and each node \var{v}, a total of $O(p^2\log p\log(p+q_{max}))$ steps
are performed by all processes during their $r$th helping phase on \var{v}.
We can amortize these steps over the operations that appear in 
$\var{v}.\fld{blocks}[(r-1)p^2+1..p^2]$.
By \Cref{blockNotEmpty}, there are at least $G$ such operations, 
so each operation's amortized number of steps for GC at each node along the path from its leaf to the root
is $O(p^2\log p\log(p+q_{max})/G)=O(\log(p+q_{max}))$.
Hence each operation's amortized number of GC steps is $O(\log p\log(p+q_{max}))$, which is
within a constant factor of the steps it performs excluding GC.
\end{proof}

We remark that the implementation remains wait-free:  the depth of recursion in each routine is  bounded
by the height of the tree and the only loop is the counted loop in the \op{Help} routine.

============

In our design a process attempts to collect the garbage, when it is going to to append a block in the $kp^2$ position in \nf{root.blocks} (see Line \ref{}). Every $p^2$  block appended to the root, one \nf{GarbageCollect} terminates because \nf{root.head} cannot advance until a \nf{GarbageCollect} garbage collect is done (see Lines ).

%\begin{lemma}
%Size of \nf{root.blocks} after \nf{GarbageCollect} is $O(p^2+q)$.
%\end{lemma}
%
%\begin{lemma}
%Total number of the blocks in the tree is $O(p^3+pq)$.
%\end{lemma}

\nf{Enqueue} operation $e$ can be removed from the tree after termination of \nf{Dequeue} $d$ where $Resp(d)=e$. It is safe to remove \nf{Dequeue} $d$ from the tree after the $d$ terminates. We can remove a block after the conditions told are satisfied for all of its \nf{Enqueue}s and \nf{Dequeue}s. A \nf{Dequeue} may go to sleep for a long time and prevent a \nf{block} to be removed (the block it is in or the block its response is in). In that situation other processes can help the \nf{Dequeue} by computing its response and writing it down somewhere. After writing down the response of th \nf{Dequeue} can be removed, since if the process wanted the response it can read from the helped response.

\begin{definition}
A block  is \it{finished} if all of its \nf{Enqueue}s have been computed to be the response of some \nf{Dequeue} and all of its \nf{Dequeues} responses have been computed.  
\end{definition}

\begin{corollary}
If a block is finished, then all of its subblocks are also finished.  
\end{corollary}

\begin{lemma}
It is safe to remove a finished block from the non-leaf nodes.
\end{lemma}

If the $i$th \nf{Enqueue} gets dequeued in a FIFO queue, it means the first $i-1$ \nf{Enqueue} operations have been already dequeued. This gives us the idea that if a block is finished then all the blocks before it are also finished. If an operation in a block goes to sleep for a long time then other processes help the operation so the block gets finished. There are less than $p$ idle operations, we can help them before garbage collection and then remove all the finished blocks safely in an amortized poly-log time.

\begin{lemma}
    If all \nf{Dequeue} operations in the root are helped, then all the blocks before a finished block in the root are also finished.
\end{lemma}

\begin{lemma}
  Blocks before the most recent block that has been dequeued(computed to be dequeue) from are finished. If the most current \nf{Dequeue} returned(computed) \nf{null} then all the blocks before the block containing the \nf{Dequeue} are finished.
\end{lemma}

\begin{lemma}
    If we replace the arrays we used to implement \nf{blocks} with red-black trees the amortized complexity of the algorithm would be $PolyLog(p,q)$. And also the algorithm is correct.
\end{lemma}

We can help a \nf{Dequeue} by computing its response and writing it down. If the process in future failed to execute, it can read the helped value written down.

\begin{lemma}
The \nf{response} written is correct.
\end{lemma}

But how can we know which blocks in each node are finished or not? 


\begin{lemma}
  Algorithm is wait-free and linearizable.
\end{lemma}
