% !TEX root =  podc-submission.tex

\section{Related Work}

Michael and Scott \cite{MS98} describe the early history of concurrent queue algorithms
in the paper that described their MS-queue, which
is a lock-free queue that has stood the test of time.
A version of it is
included in the standard Java Concurrency Package.  %java.util.concurrent.ConcurrentLinkedQueue
However, as mentioned above, it suffers from the CAS retry problem
which means it is lock-free but not wait-free and has an amortized step complexity
of $\Theta(p)$ per operation.

A number of papers have described ways to reduce contention at the front
and back of the MS-queue.
Moir et al.~\cite{DBLP:conf/spaa/MoirNSS05} 
added an elimination array that allows an enqueue to pass its enqueued value directly
to a concurrent dequeue when the queue is empty or when concurrent operations 
can be linearized to empty the queue.
However, when there are $p$ concurrent enqueues (and no dequeues), the CAS retry problem
is still present.
The baskets queue of
Hoffman, Shalev, and Shavit~\cite{DBLP:conf/opodis/HoffmanSS07} 
is another approach to avoid contention:  concurrent enqueues are grouped into baskets.
If an enqueue fails its CAS, it is put in the basket with the operation that succeeded,
and operations within a basket can be ordered arbitrarily.
However, if there are $p$ concurrent enqueues in the same basket
the CAS retry problem occurs when they order themselves into a list using CAS instructions.
Both modifications thus still have $\Omega(p)$ amortized step complexity.


- MKLL18 batches several of a process's operations together and apply them as a group to the queue but this assumes processes can wait to do a batch all at once.  In worst case batches of size 1 reduces to MS98
- RC17 sketched how to add a helping mechanism to ensure wait-freedom (increases amortized time to Omega($P^2$) for enqueue)
- Kogan Petrank PPoPP 2011 add helping to MS98 queue to make it wait-free; only makes complexity worse

LS08 modified MS98; switched to doubly-linked list to reduce number of successful CASes needed for an op from 2 to 1 in optimistic approach, but could get inconsistent pointers that can be fixed when needed.  Fixing the list, though rare in practice, is expensive, so Omega(nP) amortized time per op.

-----

Ladan-Mozes and Shavit~\cite{DBLP:journals/dc/Ladan-MozesS08}
presented an optimistic approach to implement a queue. MS-queue uses
two \nf{CAS}es to do an enqueue: one to change the tail to the new
node and another one to change the next pointer of the previous node
to the new node. They use a doubly-linked list to do fewer
\texttt{CAS} operations in an \nf{Enqueue} than MS-queue. As in
previous algorithms, the worst case happens in the case where the
contention is high: when $p$ concurrent enqueues happen, their nodes
have to be appended to the linked list one by one. The amortized
complexity is still $\Omega(p)$ \texttt{CAS}es. 

Hendler et al.~\cite{DBLP:conf/spaa/HendlerIST10} proposed a new
paradigm called flat combining. The key idea behind flat combining is
to allow a combiner who has acquired the global lock on the data
structure to learn about the requests of threads on the queue, combine
them and apply the combined results on the data structure. Their queue
is linearizable but not lock-free and they present experiments that
show their algorithm performs well in some situations. 

Gidenstam, Sundell, and Tsigas~\cite{DBLP:conf/opodis/GidenstamST10}
introduced a new algorithm using a linked list of arrays. The queue is
stored in a shared array where head and tail pointers point to the
current elements in the queue. When the array is full, an empty array
is linked to the array and tail pointers are updated. A global head
points to the array containing the first element in the queue, and
each process has a local head index that points to the first element
in that array. Global tail and local tail pointers are similar. A
process updates the position of the pointers after it does an
operation. One process might go to sleep before setting the pointers,
so the pointers might be behind their real places. They mention how to
scan the arrays to update pointers while doing an operation. A process
writes an element in the location head by a \nf{CAS} instruction, so
if $p$ processes try to enqueue simultaneously, the amortized step
(and \nf{CAS}) complexity remains $\Omega(p)$. Their design is
lock-free but not wait-free. 

Kogan and Petrank~\cite{DBLP:conf/ppopp/KoganP11} introduced wait-free
queues based on the MS-queue and use Herlihy's helping
technique~\cite{10.1145/114005.102808} to achieve wait-freedom. Their
amortized step complexity is $\Omega(p)$ because of the helping
mechanism. 

Milman-Sela et al.~\cite{MKLLP22} designed a lock-free
queue supporting futures. In their queue, operations  return future
objects instead of responses. Later when the response is needed, it
can be evaluated from the future object. They also define a weaker
linearizability condition such that each operation can be linearized
between its invocation and when its future is evaluated. Their idea of
batching allows a sequence of operations to be submitted as a batch
for later execution on the MS-queue. They use some properties of the
queue size before and after a batch, similar to a part of our
work. Their queue is not wait-free: in fact, if the batch sizes are 1,
then the queue is like MS-queue. 

Nikolaev and Ravindran~\cite{10.1145/3490148.3538572} present a
wait-free queue that uses the fast-path slow-path methodology
introduced by Kogan and Petrank~\cite{10.1145/2370036.2145835}. Their
work is based on a circular queue using bounded memory. When a process
wishes to do an enqueue or a dequeue, it starts two paths. The fast
path  ensures good performance while the slow path ensures
termination. They show that these two paths do not affect each other
and the queue remains consistent. If a process makes no progress,
other processes help its slow path to finish. The helping phase
suffers from the CAS Retry Problem because processes compete in a
\nf{CAS} loop to decide which succeeds to help. Because of this, the
amortized complexity cannot be better than $\Omega(p)$. 

The CAS Retry Problem is not limited to list-based queues; array-based
queues also share
it~\cite{DBLP:conf/iceccs/ColvinG05,DBLP:conf/icdcn/Shafiei09,DBLP:conf/spaa/TsigasZ01}.
Our motivation is to overcome this problem and present a wait-free
sublinear queue. 

\here{Check:
Turek Shasha Prakash locking without blocking (PODS92).
Prakash Lee Johnson non blocking algorithms for concurrent data structures Tech rept 91-002 U of Florida 1991.
}

\paragraph{Other Primitives and Restricted Queues}

David introduced the first sublinear-time queue
\cite{DBLP:conf/wdag/David04}, but it works only for a single enqueuer.
It uses fetch\&increment and swap primitive instructions and takes constant time per operation, but
uses unbounded memory.  A modification to use bounded space
is described, but it increases the time per operation to $\Omega(p)$.

Jayanti and Petrovic introduced a wait-free poly-logarithmic
queue~\cite{DBLP:conf/fsttcs/JayantiP05}, but it only works for a single dequeuer. 
Our implementation uses their idea of having
a tournament tree among processes to agree on the linearization of
operations.

\here{new result I found:}
Khanchandani and Wattenhofer \cite{KW18} gave a wait-free queue implementation
with $O(\sqrt{p})$ complexity using non-standard synchronization primitives
called half-increment and half-max, which can be viewed as particular kinds of
double-word read-modify-write operations.
They use this as evidence that their primitives can be more efficient than CAS
since previous CAS-based queues all required $\Omega(p)$ step complexity.
Our new implementation counters this argument.


\subsection{Universal Constructions and Other Poly-log Time Data Structures}
A \textit{universal construction} is an algorithm that can implement a
shared version of any given sequential object. The first universal
construction was introduced by
Herlihy~\cite{10.1145/114005.102808}. We can implement a concurrent
queue using a universal construction. Jayanti proved an $\Omega(\log
p)$ lower bound on the worst-case shared-access time complexity of
$p$-process universal
constructions~\cite{DBLP:conf/podc/Jayanti98a}. He also mentions that
the universal construction by Afek, Dauber, and
Touitou~\cite{DBLP:conf/stoc/AfekDT95} can be modified to $O(\log p)$
worst-case step complexity, using atomic access to $\Omega(p \log
p)$-bit words. Chandra, Jayanti and Tan introduced a semi-universal
construction that achieves $\textsc{O}(\log^2 p)$ shared
accesses~\cite{DBLP:conf/podc/ChandraJT98}. However, their algorithm
cannot be used to create a queue. We mention a non-practical universal
construction with a poly-log number of \nf{CAS} instructions in the
last paragraph of page 13. 

Ellen and Woelfel introduced an implementation of a Fetch\&Inc object
with step complexity of $O(\log p)$ using $O(\log n$)-bit
\texttt{LL/SC} objects, where $n$ is the number of
operations~\cite{10.1007/978-3-642-41527-2_20}. Their idea to achieve
logarithmic complexity is to use a tree storing the Fetch\&Inc
operations invoked by processes. When a process wants to do a
Fetch\&Inc it adds its Fetch\&Inc to the tree and returns the number
of elements in the tree. There are some similarities between designing
a queue and a Fetch\&Inc object. A Fetch\&Inc object can be
constructed from a queue. The algorithm by Ellen and Woelfel is
interesting because of the similarities between Fetch\&Inc objects and
queues. Also, it is one of the few wait-free data structures achieving
poly-logarithmic complexity. 

