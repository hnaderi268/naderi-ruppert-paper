% !TEX root =  podc-submission.tex

\section{Related Work}

\paragraph{List-based Queues}
The MS-queue \cite{MS98} is a lock-free queue that has stood the test of time.
A version of it is
included in the standard Java Concurrency Package.  %java.util.concurrent.ConcurrentLinkedQueue
See the paper that introduced it for a survey of the early history of concurrent queues.
As mentioned above, the MS-queue suffers from the CAS retry problem because of contention at the front and back of the queue.
Thus, it is lock-free but not wait-free and has an amortized step complexity
of $\Theta(p)$ per operation.

A number of papers have described ways to reduce contention in the MS-queue.
Moir et al.~\cite{DBLP:conf/spaa/MoirNSS05} 
added an elimination array that allows an enqueue to pass its enqueued value directly
to a concurrent dequeue when the queue is empty or when concurrent operations 
can be linearized to empty the queue.
However, when there are $p$ concurrent enqueues (and no dequeues), the CAS retry problem
is still present.
The baskets queue of
Hoffman, Shalev, and Shavit~\cite{DBLP:conf/opodis/HoffmanSS07} 
attempts to reduce contention by grouping concurrent enqueues into baskets.
An enqueue that fails its CAS is put in the basket with the eqnueue that succeeded.
Enqueues within a basket determine their order among themselves without having to access the back of the queue.
However, if $p$ concurrent enqueues are in the same basket
the CAS retry problem occurs when they order themselves using CAS instructions.
Both modifications still have $\Omega(p)$ amortized step complexity.

Kogan and Herlihy \cite{DBLP:conf/podc/KoganH14} described how to use futures to improve
the performance of the MS-queue.
Operations return future
objects instead of responses. Later, when an operation's response is needed, it
can be evaluated using the corresponding future object.
This allows batches of enqueues or dequeues to be done at once on an MS-queue.
However, the implementation satisfies a weaker correctness condition than linearizability.
Milman-Sela et al.~\cite{MKLLP22} extended this approach to allow batches
to mix enqueues and dequeues.
\here{Is this worth saying:
They use some properties of the queue size before and after a batch, similar to a part of our work.}
In the worst case, where operations require their response right away,
batches have size 1, and both of these implementations behave like a standard MS-queue.

Ladan-Mozes and Shavit~\cite{DBLP:journals/dc/Ladan-MozesS08}
presented an optimistic  queue implementation. 
%MS-queue uses
%two \nf{CAS}es to do an enqueue: one to change the tail to the new
%node and another one to change the next pointer of the previous node
%to the new node. 
In the MS-queue, an enqueue requires two CAS steps.
The optimistic queue uses a doubly-linked list to reduce the number of
\texttt{CAS} instructions to one in the best case. 
Pointers in the doubly-linked list can be inconsistent, but are fixed when needed by traversing the list.
Although this fixing is rare in practice, it yields an amortized complexity of $\Omega(qp)$ 
steps per operation for worst-case executions.

Kogan and Petrank~\cite{DBLP:conf/ppopp/KoganP11} 
added Herlihy's helping
technique~\cite{10.1145/114005.102808} to the MS-queue to obtain
a wait-free queue.
Ramalhete and Correia \cite{RC17} added a different helping mechanism.
In both cases, the helping mechanisms only add to the step complexity of the MS-queue.


-----

% OMIT, since it is not lock-free:
%Hendler et al.~\cite{DBLP:conf/spaa/HendlerIST10} proposed a new
%paradigm called flat combining. The key idea behind flat combining is
%to allow a combiner who has acquired the global lock on the data
%structure to learn about the requests of threads on the queue, combine
%them and apply the combined results on the data structure. Their queue
%is linearizable but not lock-free and they present experiments that
%show their algorithm performs well in some situations. 

Gidenstam, Sundell, and Tsigas~\cite{DBLP:conf/opodis/GidenstamST10}
introduced a new algorithm using a linked list of arrays. The queue is
stored in a shared array where head and tail pointers point to the
current elements in the queue. When the array is full, an empty array
is linked to the array and tail pointers are updated. A global head
points to the array containing the first element in the queue, and
each process has a local head index that points to the first element
in that array. Global tail and local tail pointers are similar. A
process updates the position of the pointers after it does an
operation. One process might go to sleep before setting the pointers,
so the pointers might be behind their real places. They mention how to
scan the arrays to update pointers while doing an operation. A process
writes an element in the location head by a \nf{CAS} instruction, so
if $p$ processes try to enqueue simultaneously, the amortized step
(and \nf{CAS}) complexity remains $\Omega(p)$. Their design is
lock-free but not wait-free. 



Nikolaev and Ravindran~\cite{10.1145/3490148.3538572} present a
wait-free queue that uses the fast-path slow-path methodology
introduced by Kogan and Petrank~\cite{10.1145/2370036.2145835}. Their
work is based on a circular queue using bounded memory. When a process
wishes to do an enqueue or a dequeue, it starts two paths. The fast
path  ensures good performance while the slow path ensures
termination. They show that these two paths do not affect each other
and the queue remains consistent. If a process makes no progress,
other processes help its slow path to finish. The helping phase
suffers from the CAS Retry Problem because processes compete in a
\nf{CAS} loop to decide which succeeds to help. Because of this, the
amortized complexity cannot be better than $\Omega(p)$. 

The CAS Retry Problem is not limited to list-based queues; array-based
queues also share
it~\cite{DBLP:conf/iceccs/ColvinG05,DBLP:conf/icdcn/Shafiei09,DBLP:conf/spaa/TsigasZ01}.
Our motivation is to overcome this problem and present a wait-free
sublinear queue. 

\here{Check:
John Giacomoni et al. FastForward
https://dl-acm-org.ezproxy.library.yorku.ca/doi/10.1145/1345206.1345215
}

\paragraph{Other Primitives and Restricted Queues}

David introduced the first sublinear-time queue
\cite{DBLP:conf/wdag/David04}, but it works only for a single enqueuer.
It uses fetch\&increment and swap primitive instructions and takes constant time per operation, but
uses unbounded memory.  A modification to use bounded space
is described, but it increases the time per operation to $\Omega(p)$.

Jayanti and Petrovic introduced a wait-free poly-logarithmic
queue~\cite{DBLP:conf/fsttcs/JayantiP05}, but it only works for a single dequeuer. 
Our implementation uses their idea of having
a tournament tree among processes to agree on the linearization of
operations.

\here{new result I found:}
Khanchandani and Wattenhofer \cite{KW18} gave a wait-free queue implementation
with $O(\sqrt{p})$ step complexity using non-standard synchronization primitives
called half-increment and half-max, which can be viewed as particular kinds of
double-word read-modify-write operations.
They use this as evidence that their primitives can be more efficient than CAS
since previous CAS-based queues all required $\Omega(p)$ step complexity.
Our new implementation counters this argument.


\subsection{Universal Constructions and Other Poly-log Time Data Structures}
A \textit{universal construction} is an algorithm that can implement a
shared version of any given sequential object. The first universal
construction was introduced by
Herlihy~\cite{10.1145/114005.102808}. We can implement a concurrent
queue using a universal construction. Jayanti proved an $\Omega(\log
p)$ lower bound on the worst-case shared-access time complexity of
$p$-process universal
constructions~\cite{DBLP:conf/podc/Jayanti98a}. He also mentions that
the universal construction by Afek, Dauber, and
Touitou~\cite{DBLP:conf/stoc/AfekDT95} can be modified to $O(\log p)$
worst-case step complexity, using atomic access to $\Omega(p \log
p)$-bit words. Chandra, Jayanti and Tan introduced a semi-universal
construction that achieves $\textsc{O}(\log^2 p)$ shared
accesses~\cite{DBLP:conf/podc/ChandraJT98}. However, their algorithm
cannot be used to create a queue. We mention a non-practical universal
construction with a poly-log number of \nf{CAS} instructions in the
last paragraph of page 13. 

Ellen and Woelfel introduced an implementation of a Fetch\&Inc object
with step complexity of $O(\log p)$ using $O(\log n$)-bit
\texttt{LL/SC} objects, where $n$ is the number of
operations~\cite{10.1007/978-3-642-41527-2_20}. Their idea to achieve
logarithmic complexity is to use a tree storing the Fetch\&Inc
operations invoked by processes. When a process wants to do a
Fetch\&Inc it adds its Fetch\&Inc to the tree and returns the number
of elements in the tree. There are some similarities between designing
a queue and a Fetch\&Inc object. A Fetch\&Inc object can be
constructed from a queue. The algorithm by Ellen and Woelfel is
interesting because of the similarities between Fetch\&Inc objects and
queues. Also, it is one of the few wait-free data structures achieving
poly-logarithmic complexity. 

